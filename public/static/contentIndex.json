{"Affine-Transformation":{"title":"Affine Transformation","links":[],"tags":["작성중"],"content":"this is for test"},"Inductive-Bias":{"title":"Inductive Bias","links":[],"tags":["작성중"],"content":""},"K-means-Clustering":{"title":"K-means Clustering","links":[],"tags":["작성중"],"content":""},"Manim/사용법":{"title":"사용법","links":[],"tags":[],"content":"python -m manim path.py className\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncmd기능-pplay-pqlplay &amp; quality=480p15-pqmplay &amp; quality=720p30-pqhplay &amp; quality=1080p60\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython기.shift대상 이동"},"Manim/사용법":{"title":"사용법","links":[],"tags":[],"content":"python -m manim path.py className\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncmd기능-pplay-pqlplay &amp; quality=480p15-pqmplay &amp; quality=720p30-pqhplay &amp; quality=1080p60\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython기능.shift대상 이동"},"Softmax":{"title":"Softmax","links":[],"tags":["작성중"],"content":""},"Templates/model":{"title":"model","links":[],"tags":[],"content":"1. Architecture\n2. Training"},"Words":{"title":"Words","links":[],"tags":[],"content":"TARGET DECK\nComputer Science\ncontextual :: 문맥과 관련된\n\nsemantic :: 의미의\n\npolysemy :: 다의성\n\nentailment :: 함의\n\nin sense that :: ~라는 점에서\n\nmarkedly :: 현저히\n\nextensive :: 광범위한\n\noutperforms :: 능가하다\n\nseamless :: 원활한\n\nincorporate :: 포함하다\n\nplentiful :: 풍부한\n\nleverage :: 영향력\n\nintrinsic :: 본질적인, 내부의\n\npractical :: 실질적인\n\nexplicit :: 명백한\n\nincorporate :: 통합하다\n\ndictate :: 지시하다\n\nleverage :: 활용하다\n\nanalogous :: 유사한\n\nimplement :: 구현하다\n\ncombine :: 결합하다\n\ndepart :: 출발하다\n\ninclusion :: 포함\n\ncoreference :: 상호 참조\n\nmoderate amount :: 적당한 양\n\nimpose :: 부과하다\n\ninductive :: 유도적인\n\nperplexity :: 복잡성\n\nhalved :: 반감시키다\n"},"index":{"title":"index","links":[],"tags":[],"content":"Welcome! 🌱\n\n  Take a look at [[Your first note]] to get started on your exploration.\n\nThis digital garden template is free, open-source, and available on GitHub here.\nThe easiest way to get started is to read this step-by-step guide explaining how to set this up from scratch.\nRecently updated notes\n\n  .wrapper {\n    max-width: 46em;\n  }\n"},"개인연구/Scanner-CNN":{"title":"Scanner-CNN","links":["정보과학/Machine-Learning/Architecture/Convolutional-Layer","Softmax"],"tags":[],"content":"EEG 분류 과정에서 CNN을 많이 사용하는데, CNN의 kernel은 위치를 고려하기 위함이기 때문에 의미가 없을 것 같다. 따라서 kernel size를 채널 수에 맞춰버리자.\n1. Architecture\nConvolutional Layer\nSoftmax"},"개인연구/parameter-optimization-to-Weber's-problem":{"title":"parameter optimization to Weber's problem","links":[],"tags":[],"content":"기하 중앙값\n주어진 점들에 대해서 유클리디안 거리의 합이 최소가 되는 지점\n오목 함수\n한 점이 주어질 때 점의 위치에 따른 거리가 오목 함수이고 오목 함수의 합은 오목 함수 이므로 optimization이 매우 간단하다.\nsingle layer perceptron으로 이루어진 model이 있다.각 데이터에 대해 overfitted model이 있을 때, 이 model의 parameter W,b를 해당 데이터에 대한 일종의 representation으로 생각해보자. 각 데이터 Di​에 대한 prediction error를 최소화하는 parameter를 Wi​,bi​이라고 하자. 전체 데이터를 학습시킨 모델의 parameter를 Wmodel​,bmodel​라고 할 때, model이 가지는 error는 ∑∣Wi​−Wmodel​∣, ∑∣bi​−bmodel​∣ 각각에 대해 단조 증가 함수이다. 물론 activation function에 따른 차이가 있겠지만, 적어도 감소하진 않는다(예를 들어, ReLU에서는 입력이 일정 값 이하이면 기울기가 0이 된다).\n그렇다면, Weber’s problem 또는 geometric median에서 그러했듯이, optimal parameter는 각 데이터에 대한 Wi​,bi​와의 차이가 가장 작은 점이 아닐까?\n1. Solutions to Weber’s problem\n1.1 Weiszfeld Algorithm\n\n초기 추정값을 설정한다.\ny0​=mean(x)\n\n오차의 크기에 반비례하여 각 점에 가중치를 둔다. 이때 가중치의 역수의 합으로 나누어 정규화를 한다.\n\nyk+1​=(∑i=1m​∣∣xi​−yk​∣∣+ϵ1​)(∑i=1m​∣∣xi​−yk​∣∣+ϵxi​​)​(ϵ=1e−5)\n식을 보면, machine learning에서 일반적으로 사용하는 squared error가 아니다. 이러한 방식이 MSE에서도 통할까? 굳이 MSE를 쓰지 말고, 위 식을 조금 수정하여 RMSE 형태로 사용해보자. MSE가 지나치게 커지는 문제를 해결할 수 있을 것이다.\nRMSEk​=yk+1​=​i=1∑m​∣∣xi​−yk​∣∣+ϵ1​⋅n1​​(ϵ=1e−5)∣∣RMSEk​∣∣RMSEk​⋅x​​​\n앞에서, parameter에 따른 error가 단조 증가 함수라고 서술하였다. 이 때문에 발생하는 몇 가지 문제가 있다. ReLU를 예로 들어보면, 입력 값이 일정 이하일 때 0을 반환하기 때문에, error가 동일한 구간이 생기게 된다. "},"개인연구/주제":{"title":"주제","links":["정보과학/Machine-Learning/Architecture/Fully-Connected-Layer","정보과학/Machine-Learning/model/Support-Vector-Machine","정보과학/Machine-Learning/Architecture/Spatial-Pyramid-Pooling","개인연구/Scanner-CNN","개인연구/parameter-optimization-to-Weber's-problem"],"tags":[],"content":"개인 연구\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n번호주제1[[Optimizer#gradient-descent2머신러닝, 유전 알고리즘 등을 통한 작도3회로 기반 [[Optimizer#gradient-descent4Genetic Algorithm 사용할 때 탐색의 후보 중 일부가 이전 세대로 부터 넘어오는데, 이중 일부를 적합도에 따른 history에서 상위에 rank한 개체로 선택하기5optimization에서 사용되는 error는 결국 각 정답에 대한 오차 공간을 summation한 거니까, 정답 공간에서 정답의 밀도가 높은 구간을 찾아내는 게 global minima에 근접하는 데에 도움을 주지 않을까? → error는 결국 정답 분포의 weighed sum?6Fully Connected Layer 이후 Support Vector Machine을 통해 classification을 수행하는 경우가 많은데, 그러면 fully connected layer는 결국 support vector machine을 위한 공간을 구성하는 역할?7EEG 할 때 CNN 쓰는 경우 많이 봤는데, 얘는 옆 채널과 직접적인 연관이 없는 경우가 있기에, kernel을 사용하는 것이 적합하지 않을지도 모른다. scanning하는 것 처럼 kernel의 세로 길이를 채널 수에 맞추어 embedding vector 만드는 형식이 나을 수도 있겠다. Spatial Pyramid Pooling 처럼 kernel 가로 길이 조절해도 좋고.Scanner-CNN8각 샘플에 대한 최적의 parameter들을 찾으면 global optima를 이루는 parameter를 찾는 것은 parameter들에 대한 Weber problem으로 볼 수 있지 않을까?parameter optimization to Weber’s problem\n토이 프로젝트\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n번호주제1LLM으로 노트 필기 검증2이디저디 특별실 인원 수정 기능 추가3실시간 수업 이해도 피드백4인스타그램 네트워크 빌더 업데이트5바디캠으로 손이 물체에 가까워지면 팔찌 등으로 진동을 통해 알려주기\n몰루\n\n\n\n\n\n\n\n\n\n\n\n\n\n번호주제1동형 암호 기반 드론 촬영물 모자이크"},"무제-파일":{"title":"무제 파일","links":[],"tags":[],"content":""},"백준/못-푼-문제":{"title":"못 푼 문제","links":[],"tags":[],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n번호분류1418번수학, 브루트포스 알고리즘, 정수론, 소수 판정, 에라토스테네스의 체1697번그래프 이론, 그래프 탐색, 너비 우선 탐색1713번구현, 시뮬레이션1932번다이나믹 프로그래밍2630번분할 정복, 재귀10875번구현, 시뮬레이션11660번다이나믹 프로그래밍, 누적 합14501번다이나믹 프로그래밍, 브루트포스 알고리즘"},"생명과학/Cognitive-Science/Cognitive-Psychology/주의(attention)":{"title":"주의(attention)","links":[],"tags":[],"content":"선택적 주의(selective attention)\n우리의 환경이나 기억에서 우리가 앞으로 처리하거나 생각하고 싶은 무언가를 선택하는 데 필요한 인지 자원으로 정의된다.\n지속적 주의(sustained attention)\n한 순간에서부터 다음 순간까지 동일한 사고나 과제에 관여하는 데 필요한 인지 처리의 결과로 정의된다.\n집중적 주의(focused attention)\n무언가를 계속 주의하기 위한 의식적 노력에 기대는 과정이라고 정의된다."},"생명과학/Developmental-Biology/Actin":{"title":"Actin","links":[],"tags":["작성중"],"content":"Filament Actin\nGlobular Actin"},"생명과학/Developmental-Biology/Cell-Division":{"title":"Cell Division","links":[],"tags":["작성중"],"content":""},"생명과학/Developmental-Biology/Differentiation":{"title":"Differentiation","links":[],"tags":["작성중"],"content":""},"생명과학/Developmental-Biology/Initiate-Embryonic-Development":{"title":"Initiate Embryonic Development","links":["생명과학/Developmental-Biology/Model-Organism"],"tags":[],"content":"1. Sea urchin\nexternal fertilization을 하기 때문에 발생 과정을 관찰하기 쉬워 Model Organism로서 많이 사용된다.\n1.1. Jelly coat\n난자의 jelly coat에서 방출된 resact가 sperm cell의 receptor guanilyl cyclase에 결합하여 GTP를 cGMP로 치환시키고 이는 calcium channel을 개방하여 세포 내의 Ca2+의 농도를 증가시켜 sperm이 resact의 농도가 높은 곳을 따라 움직이게 한다.\n1.2. Acrosomal Reaction\n\n\n\nsprem의 머리 부분이 egg cell의 jelly layer 표면에 닿는다.\njelly coat의 분자들에 의해서 acrosomal의 exocytosis가 촉진된다.\nhydrolytic enzyme이 jelly coat를 분해한다.\nacrosome이 돌기 형태로 신장되어 acrosomal process를 형성하고 jelly coat를 관통한다.\nacrosome process 끝에 있는 bindin이 egg cell membrane의 bindin receptor에 종 특이적으로 결합한다.\nacrosomal process와 egg의 plasma membrane이 된다.\nsperm의 핵이 egg의 세포질에 들어간다.\negg의 세포막의 ion channel들이 개방된다.\nsodium ion이 egg 안으로 확산되며 탈분극이 일어난다.\n전위차가 감소해 약 1분간 fast polyspermy block이 일어난다.\n\n1.3. Cortical Reaction\n\n\nsperm이 egg에 결합하여 cortical granules가 egg의 plasma membrane과 융합된다.\ncortical granules에 들어있던 물질들이 방출되어 egg plasma membrane과 vitelline layer 사이로 방출된다.\ncortical granules에서 방출된 물질들이 vitelline envelope에 결합하여 fertilization envelope을 형성한다.\nfertilization layer가 다른 sperm의 acrosomal process가 egg cell membrane에 융합하는 것을 방지한다.\n\n1.3.1. Ca2+ in Cortical Reaction\nCa2+는 vesicle이 plasma membrane에 융합하는 것을 조절한다.\nex) neurotransmitter release, insulin secretion, plant pollen tube formation\nfree Ca2+에 결합하는 fluorescence dye를 fertilizing egg에 주입하여 관찰한다.\n\ncytosol에서 sperm이 들어온 지역으로부터 [Ca2+]cytoplasm​의 농도가 점차 높아진다.\nCa2+의 농도가 증가하는 propagation이 일어난다.\nfertilization envelop을 형성할 때 사용되는 Ca2+는 어디서 유래하는가?\n\nCa2+가 없는 바닷물에 A23187을 처리한 뒤 Sea urchin egg 넣었을 때 ER에서 방출된 Ca2+에 의해 cortical granule이 egg의 plasma membrane과 결합하였다. 하지만 이는 A23187을 넣었기 때문에 ER의 Ca2+가 방출되어 사용된 것이지, 실제로는 어떨지 모른다.\n"},"생명과학/Developmental-Biology/Model-Organism":{"title":"Model Organism","links":[],"tags":["작성중"],"content":"다른 생물의 발생 과정을 알아보기 위해 간접적으로 사용된다."},"생명과학/Developmental-Biology/Vanishing-Twin":{"title":"Vanishing Twin","links":[],"tags":[],"content":""},"생명과학/Molecular-Biology/CRISPR-Cas9":{"title":"CRISPR-Cas9","links":[],"tags":[],"content":"CRISPR(Clusterd Regularly Interspaced Short Palindremic Repeat)\n규칙적인 간격을 갖고 나타나는 짧은 palindrome의 반복을 말한다. pre-crRNA로 전사된 후 침입한 표적 RNA or DNA를 파괴하는 짧은 RNA로 가공된다.\nCas9\nCas enzyme은 guide RNA와 짝을 이뤄 target DNA의 이중나선을 절단한다.\nCRISPR system은 기작에 관여하는 Cas 단백질이 여러 개인 경우 class 1, 한 개인 경우 class 2로 분류한다. 두 class는 genome에서의 위치 작용하는 Cas 단백질의 종류에 따라 다시 6가지 type으로 세분화되며 CRISPR-Cas9은 class 2 type II에 해당한다.\nGuide RNA\n세포 내에서 Cas9 protein과 crRNA가 발현되면 target DNA 서열을 찾기 위한 DNA surveillance complex를 형성한다.\n\ntracrRNA라는 RNA가 추가적으로 발현되어 crRNA 내 반복 서열과 상보적인 결합을 이룬다. 이때 crRNA, linker, tracrRNA로 구성된 single guide RNA를 사용하여도 효소 활성에 문제가 없다.\nguide DNA가 DNA와 상보적인 결합을 형성하고 DNA의 상보적인 서열 근처에 PAM(Protospacer-Adjacent Motif)라는 염기 서열이 있어야만 활성을 가진다.\nCas9 단백질에는 HNH와 RuvC라는 두 개의 nuclease domain을 가지고 있는데, 목표로 하는 DNA 서열에 Cas9이 결합하면 이 두 도메인이 목표 DNA의 이중 가닥을 각각 한 가닥씩 자르게 된다.\n절단된 DNA는 DNA repair 기작을 통해 편집이 이뤄진다. 절단된 부위를 단순히 이어붙이는 비상동말단결합(Nonhomologous end joining, NHEJ) 기작으로는 유전자를 녹아웃시킬 수 있고, 상동 재조합(Homologous recombination)의 얼개인 동형방식수선(Homology Directed Repair, HDR)을 통해 유전자 단일 가닥 일부를 없애, dna polymerase와 수선 단백질로 교정하거나 삽입할 수도 있다.\n"},"생명과학/Molecular-Biology/Gene-Expression":{"title":"Gene Expression","links":["생명과학/Molecular-Biology/tRNA","생명과학/Molecular-Biology/Regulation-of-Transcription-Initiation","생명과학/Molecular-Biology/Post-Transcriptional-Modification"],"tags":[],"content":"1. Prokaryotic Cell\n1) Transcription\nRNA Polymerase\nProkaryotic Cell의 RNA Polymerase는 core enzyme(α2​ββ′ω)과 σ factor로 구성되며 이를 통틀어 holoenzyme이라고 부른다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubunitfunctionαcore enzyme 형성에 관여βNTP 결합 부위β′DNA 결합 부위ωin vitro에서는 ω subunit이 없어도 전사 가능σ-35(5’-TATAAA-3’) 및 -10(5’-TTGACA-3’) 공통 서열(consensus sequence)에 특이적으로 결합약 10nt 전사 후 방출질소원 고갈 등 특수한 상황에서 발현되는 특수 전사 인자 존재\nInitiation\n\nσ factor가 -35 region 및 -10 region에 특이적으로 결합한다.\nσ factor이 core enzyme을 -35 region에 결합시킨다.\ncore enzyme이 -35 region으로부터 내려오다가 A, T 서열이 풍부한 -10 region에 도달하면\nDNA 이중나선이 풀리고 RNA 합성이 시작된다.\n약 10nt 전사 후 σ factor가 방출된다.\n\nElongation\nTermination\nρ dependent termination\n\nATP dependent DNA-RNA helicase\n\nρ independent termination\n\nDNA의 palindrome 서열에 의해 RNA 전사체에서  Hairpin 구조 형성한다.\nrUdA 결합에 의해 RNA Polymerase가 불안정화 되고 전사가 중지된다.\nHairpin 구조에 의해 rUdA 결합이 불안정해지고 RNA 전사체가 DNA로 부터 분리된다.\n\n2) Translation\nRibosome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubunitcomponentfunctionlarge subunit34 proteins, 23s rRNA, 5s rRNA23s rRNA: peptidyl transferasesmall subunit21 proteins, 16s rRNA16s rRNA: Shine-Dalgarno Sequence에 특이적으로 결합\ntRNA\nInitiation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfactorfunctionIF ItRNA가 Ribosome의 A site에 결합하는 것을 방지IF IIGTP를 가지고 있어 large subunit과 small subunit의 결합을 도움IF IIItRNA가 결합하기 전 large subunit과 small subunit의 결합을 방지\n\nsmall subunit의 16s rRNA가 DNA에 Shine-Dalgarno Sequence에 특이적으로 결합한다.\n전사 개시 과정에서는 개시코돈 AUG에 상보적인 Met가 아닌 fMet−tRNAfMet가 P site에 결합한다.\n\nMet−tRNAformyltransferaseMet−tRNAfMet+formyl−tetrahydrofolate+→fMet−tRNAfMet+tetrahydrofolate​\n\n이후 large subunit이 결합해 Elongation phase가 시작된다.\n\nElongation\n\nEF-Tu-GTP가 AA-tRNA를 A site에 결합시키고 23s rRNa의 peptidyl transferase 활성에 의해 peptidyl-tRNA의 polypeptide에 AA가 첨가된다.\n\nEF－Tu－GTP→EF－Tu－GDP\n\nEF-Ts가 EF-Tu의 GDP와 치환되고 이는 다시 GTP와 치환된다.\n\nEF－Tu－GDP→EF－Tu－EF－Ts→EF－Tu－GTP​\n\nEF-G가 GTP를 소모하여 mRNA와 AA-tRNA를 한 코돈 만큼 이동 시킨다.\n\nEF－G－GTP→EF－G−GDP+Pi​\nTermination\n\n종결코돈(UGA, UAG, UAA) 등에 도달하면 Releasing Factor가 결합해 23s rRNA의 peptidyl transferase 활성을 조절하여 아미노산 대신 물을 첨가하여 peptidyl-tRNA의 에스터 결합을 가수분해하도록 한다.\n\nException\n\ninvitro에서 Releasing Factor 대신 AA－tRNAUAA 등이 종결코돈에 결합하면 번역이 종결되지 않는다.\n\n2. Eukaryotic Cell\n1) Transcription\nRNA Polymerase\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npolymeraselocationproductRNA polInucleolus5.8s rRNA, 18s rRNA, 28s rRNARNA polIInucleoplasmmRNA, snoRNA, miRNA, siRNA, IncRNA, 대부분의 snRNARNA polIIInucleoplasmtRNA, 5s RNA, 일부 snRNA\nInitiation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfactorfunctionTBPDNA의 -30 region의 TATA Box에 특이적으로 결합한다.TFIIDpromoter를 특이적으로 인식하여 RNA polymerase와 promoter간의 결합을 도와준다TFIIHRNA polII의 CTD를 인산화하여 인산화된 단백질 꼬리에 RNA 가공과정에 관여하는 요소들이 결합할 수 있도록 해준다.\n\nTFIID의 subunit인 TATA-binding protein이 TBP에 결합한다.\nRNA polII 및 TFIIB, TFIIE, TFIIH 등의 GTFs가 결합한다.\nTFIIH가 RNA polII의 CTD를 인산화 시켜 가공 관련 효소들이 도입되고 전사를 개시한다.\n\nRegulation of Transcription Initiation\nPost-Transcriptional Modification\n2) Translation\nRibosome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubunitcomponentlarge subunit~49 proteins, 5s rRNA, 5.8s rRNA, 28s rRNAsmall subunit~33 proteins, 18s rRNA\nInitiation\nEukaryotic Cell은 번역 개시 과정에서 Met－tRNAi​를 사용한다.\nSignal Recognition Particle\n\n번역 도중 SRP가 signal peptide에 결합해 번역을 중단시킨다.\nSRP receptor protein에 결합해 SRP와 결합한 signal peptide가 떨어진다. (signal peptide는 receptor protein complex 내의 효소에 의해 절단된다.)\npolypeptide가 소포체 막 안 쪽으로 이동하고 번역이 재개된다.\nEukaryotic Cell에서 단백질의 첫 번째 서열이 Met이 아닌 이유는 이 때문이다.\n"},"생명과학/Molecular-Biology/Post-Transcriptional-Modification":{"title":"Post-Transcriptional Modification","links":[],"tags":[],"content":"Post-Transcriptional Modification\n1. 5’ capping\nm7Gppp+pppNp→m7Gp+ppNp→m7GpppNp\n5’ 말단에 7-메틸구아닌 뉴클레오타이드가 첨가되어 5’→3’ exonuclease가 작용하지 못하도록 한다.\n핵공 복합체가 특이적으로 인식하여 세포질로 이동시킨다.\n5’-cap binding protein이 5’-말단의 cap에 결합한다.\n2. 3’ tailing\n\nmRNA 전사체의 3’ 말단으로부터 약 20nt 위에 5’-AAUAAA-3’에 특정 효소가 결합한다.\n10~35nt 아래 지점을 절단하고 3’ 말단에 아데닌 뉴클레오타이드를 첨가해 poly-A tail를 형성한다.\npoly-A tail은 RNA의 안전성을 유지하는 데에 중요한 역할을 한다.\npoly-A binding protein이 poly-A tail에 결합한다.\n\n3. splicing\nmRNA 전사체에서 일부 서열은 성숙한 mRNA가 되며 제거된다.\n이때 제거되는 부분을 intron, 나머지 부분을 exon이라고 부르며 intron을 제거하고 exon을 이어붙이는 과정을 RNA splicing이라고 한다.\n하나의 RNA 에서 어떤 영역을 exon으로 선택하느냐에 따라 다양한 polypeptide가 생성될 수 있다. 인간 단백질을 암호화하는 유전자 중 대략 90% 이상에서 선택적 RNA splicng 일어난다.\n모든 가공 과정이 끝난 mRNA는 핵공을 통해 세포질로 빠져나간다."},"생명과학/Molecular-Biology/Regulation-of-Gene-Expression":{"title":"Regulation of Gene Expression","links":["생명과학/Molecular-Biology/Regulation-of-Transcription-Initiation","생명과학/Molecular-Biology/Post-Transcriptional-Modification"],"tags":[],"content":"1. Eukaryoric Cell\n1) Differential gene expression\n세포는 모두 동일한 유전체를 지니고 있지만 세포 유형의 차이를 가져오는 것은 차등적 유전자 발현이 나타나기 때문이다.\n\n전형적인 분화된 인간 세포는 단백질을 암호화하는 유전자 중 대략 1/3 정도를 발현한다.\n한 개체에 존재하는 모든 세포는 동일한 유전 정보를 가지고 있다.\n예외) 면역계세포\n세포에서 발현되는 유전자\n\nhousekeeping gene(about 35%)\ntissue-specific gene(the others)\n\n\n\n2) Regulation of chromatin structure\n이질염색질은 염색질이 응축된 형태이기 때문에,  전사 기구가 접근하지 못한다. 진정염색질은 이질염색질에 비해 덜 응축되어 있어 전사 기구의 접근이 용이하고 비교적 전사되기 쉽다.\n한 번 메틸화된 DNA는 복제 과정에서 메틸화된 부분을 인식하기 때문에 세포 분열이 계속되더라도 메틸화가 유지된다.\nHistone tail modification\nnucleosome의 histone의 NTD는 바깥쪽으로 돌출되어 있다.\n이에 특정 작용기가 결합하여 염색질의 구조적 변화를 일으킨다.\n아세틸화\n일반적으로 염색질 구조가 느슨해지고 유전자 발현량이 증가한다.\n메틸화\n일반적으로 염색질 구조가 응축되고 유전자 발현량이 감소한다.\n일반적으로 사이토신 염기에 메틸기가 첨가된다.\nEpigenetic inheritance\n\n2003년 듀크 대학 연구팀 : 메틸기를 제공하는 화합물이 포함되지 않은 음식을 섭취한 어미 생쥐의 자손은 뚱뚱하고 노란 털을 지니고 있었고, 엽산처럼 메틸기를 제공하는 화합물이 포함된 음식을 섭취한 어미 생쥐의 자손은 얼룩덜룩한 갈색 털과 정상 체중을 가지고 있었다.\n2018년 네덜란드 겨울 기아 시기에 태어난 자손 연구 : 제 2차 세계대전 말기에 나치는 네덜란드로 식량이 수송되는 것을 전면적으로 막았으며, 당시 임신 중이었던 여성의 아이들은 출생 후 장기간에 걸쳐 건강 문제를 보였다. 미국과 네덜란드 연구팀은 기아를 경험했던 성인과 그들의 형제/자매간의 비교 연구를 진행하고 통계학적 분석을 통해 두 그룹 사이에 나타나는 특정 유전자들의 DNA 메틸화 패턴 차이가 장기간의 건강 문제를 유발한 것으로 결론지었다.\n\n3) Regulation of Transcription Initiation\n4) Post-Transcriptional Modification"},"생명과학/Molecular-Biology/Regulation-of-Transcription-Initiation":{"title":"Regulation of Transcription Initiation","links":[],"tags":[],"content":"1. Transcription Factor\n1) General Transcription Factor\n\n단백질을 암호화 하는 모든 유전자들의 전사에 필수적인 전사인자이다.\npromoter 및 promoter의 TATA box 등에 결합하거나 RNA polymerase에 결합한다.\n\n2) Specific Transcription Factor\n\n특정 시기나 특정 장소에서 발현되기 위해 필요한 전사인자이다.\n조절요소와 결합하여 유전자 발현을 조절한다.\n활성자(activator)와 억제자(repressor)가 있다.\n\n2. Transcription Control Element\n전사를 촉진하는 경우 enhancer, 억제하는 경우 silencer라고 부른다.\n1) Proximal Control Element\n\npromoter와 근접한 곳에 있다.\nLinker scanning mutation 실험\n\nHSV-I의 타이미딘 kinase 유전자(tk)의 proximal control element 확인\n\n\n\n2) Distal Control Element\n\npromoter로 부터 멀리 떨어져 있다.\npromoter의 상부(upstream)에 위치할 수도 있고 하부(downsteam)에 위치할 수도 있다. exon이나 intron 내부에 있는 경우도 있다. 예) Pax6 유전자\n\n3. 조절 과정\n\nactivator protein(혹은 repressor protein)이 distal regulatory factor에 결합한다.\nDNA 굽힘 단백질에 의해 activator가 promoter에 근접한다.\nactivator가 매개자 단백질들, GTFs 및 RNA polymeraseII와 결합하여 전사 개시 복합체를 형성한다.\n일부 활성자와 억제자는 직접적으로 전사에 영향을 줄 뿐만 아니라 간접적으로 염색질 구조에 영향을 준다. 효모와 동물세포를 이용한 실험에서 일부 활성자는 특정 유전자의 promoter 주위에 히스톤을 아세틸화시키는 단백질을 끌어와 히스톤을 아세틸화시켜 해당 유전자의 전사를 촉진한다는 사실이 알려졌다. 이와 유사하게 일부 억제자는 히스톤을 탈아세틸화시키는 단백질을 끌어와 유전자 침묵(silencing)으로 알려진 전사 감소 현상을 일으킨다. 실제 진핵생물에서는 염색질-변형 단백질의 동원이 가장 보편적인 전사 억제 기작이다.\n"},"생명과학/Molecular-Biology/tRNA":{"title":"tRNA","links":[],"tags":[],"content":"RNA 번역 과정에서 tRNA의 안티코돈이 mRNA의 코돈과 상보적으로 결합하여 polypeptide에 아미노산을 결합시키는 데에 관여한다."},"생명과학/Neuroscience/Action-Potential":{"title":"Action Potential","links":[],"tags":[],"content":"막 전위란 세포막에서 발생하는 전위차로 여기에서는 신경 세포 막 안팎의 이온의 농도 차로 인해 형성된다.\n\nPropagation\n1. 휴지 전위(resting potential)\n 신경 세포 막에는 다양한 ion channel과 pump 등이 존재한다. ion channel은 이온을 단순히 투과시키시만 하고,  ion pump는 ATP를 소모해 이온을 능동수송한다. 이러한 수송 단백질이 만드는 농도 차(농도 기울기)와 이온 간 작용하는 전기적 힘(전기적 기울기)이 평형을 이룰 때의 전위차를 휴지 전위라고 한다.\n2. 차등 전위(graded potential)\n 수상 돌기를 통해 시냅스 전 뉴런에서 자극을 전달 받은 경우 수상 돌기 가시 내의 ligand-gated ion channel와 voltage-gated ion channel이 열리며 탈분극이 일어나 막 전위가 이러한 막 전위의 상승을 차등 전위라고 한다.\n3. 활동 전위(action potential)\n 차등 전위가 축삭 둔덕에 도달 했을 때, 크기가 역치 이상이라면 축삭 둔덕 내의 voltage-gated sodium channel(VGSC)이 열리고 막 전위가 상승하게 된다. 상승한 막전위로 인해 다시 주변의 VGSC가 열리며 막 전위가 급격하게 상승하게 된다. 이렇게 역치 이상의 자극이 주어졌을 때 막 전위가 급격하게 상승하는 현상을 활동 전위라고 한다."},"생명과학/Neuroscience/Common-Spatial-Pattern":{"title":"Common Spatial Pattern","links":["정보과학/Dimension-Reduction/Principal-Component-Analysis","정보과학/Pre-Processing/Whitening-Transform"],"tags":[],"content":"common spatial pattern은 2-class EEG 신호의 특징 벡터를 추출하기 위한 방법으로 특정 class의 분산을 최대화하고 다른 class의 분산을 최소화하는 일종의 Principal Component Analysis이다. N개의 channel의 대해 T개의 sample을 얻는다고 할 때 이러한 N∗T의 행렬을 Trial의 수만큼 갖는다고 하자. 각 Class에 대해 covariance matrix을 구해보면\nC=tr(XTX)XTX​\n여기서 tr()은 대각 성분들의 합을 의미하므로 XTX의  대각 성분은 각 행에서 원소의 제곱 합이다. 이제 Whitening Transform을 하기 위해 전체 클래스의 mean covariance matrix들의 합을 syntetic이라고 정의한다.\nCtotal​=C1​ˉ​+C2​ˉ​\n다음과 같이 합성 공분산 행렬에 대해 고윳값 분해를 하고 eigen value와 eigen vector를 이용해 whitening transform 행렬을 구한다.\nCtotal​=UλUTQ=λ−1​UTSi​=QCi​ˉ​QT​\n이러한 whitening 과정을 통해 identity matrix를 covariance로 갖는 S를 얻을 수 있다. 이는 채널 간 상관관계를 줄임으로써 각 채널의 독립된 특징을 나타내도록 하는 것을 의미한다. 이러한 S에 대해 고윳값 분해를 하면\nS1​=Bλ1​BTS2​=Bλ2​BT​\n2 class 모두 whitening transform 행렬을 통해 같은 공간에 사영되었으므로, 2개의 class에서 class 1에 대해 eigen value가 가장 큰 vector는 class 2에 대해 가장 작은 eigen value를 갖는다. 이때 class 1의 eigen vector에 covariance matrix를 사영시킨다면, class 1과 유사한 covariance를 가진 신호는 증폭될 것이고 다른 신호는 세기가 감소할 것이다. 이러한 필터 W를 다음과 같이 정의한다.\nW=QTB\n필터는 아래와 같이 적용할 수 있다.\nZ=WX\n이렇게 얻은 신호 Z는 일반적으로 local maxima나 신호의 절대값, 제곱 등을 통해 특징 벡터를 얻는 데에 사용된다. 요약하자면, CSP는 mean covariance matrix을 통해 whitening transform과 principal component analysis를 수행하는 것이라 볼 수 있겠다."},"생명과학/Neuroscience/Lateral-Inhibition":{"title":"Lateral Inhibition","links":[],"tags":["작성중"],"content":" 한 영역에 있는 신경 세포가 상호 간 연결되어 있을 때 한 그 자신의 axon이나 자신과 이웃 신경세포를 매개하는 interneuron을 통해 이웃에 있는 신경 세포를 억제하려는 경향이다.\n예시\n망막\n망막의 visual pathway에서 광수용기는 bipolar cell, horizontal cell, amacrine cell과의 연결을 거쳐 ganglion cell로 수렴되어 시신경 다발을 이루는데, amacrine cell에 의해 측면 억제가 이루어진다고 제안된다. 즉, 신경세포들이 흥분하게 되면 옆에 있는 이웃 신경세포에 억제성 신경전달물질을 전달하여, 이웃 신경 세포가 덜 활성화되도록 만드는 것이다. 이는 경계를 명확히 지각하기 위한 진화적 기제로 설명되기도 한다."},"생명과학/Neuroscience/Long-Term-Potentiation":{"title":"Long Term Potentiation","links":[],"tags":[],"content":"기억과 학습 등을 설명할 때 흔히 사용되는 모델 중 하나로 반복적인 자극을 받는 경우 시냅스의 연결을 강화시킨다.\n이러한 LTP는 다음과 같은 과정을 통해 이루어진다.\n\npresynaptic neuron의 축삭돌기 말단에서 Glutamate를 방출한다.\npostsynaptic neuron의 수상돌기 가시에서 NMDA recepter와 AMPA recepter가 Glutamate와 결합하여 개방되고 Na+, Ca2+ 등의 유입이 일어나 membrane potential이 상승한다.\n차등전위가 세포체에 도달하고 축삭 부근에서 활동 전위가 형성되면서 수상 돌기 방향으로 역발화(backpropagation)가 일어난다.\nNMDAR의 이온 채널은 Mg 또는 Zn ion에 의해 막혀있지만 이러한 역발화로 인한 막 전위 상승으로 이온이 분리되면서 채널이 개방된다.\n\nNMDAR의 개방으로 인해 Na+, Ca2+ 등이 유입되고 이러한 Ca ion은 다양한 단백질의 활성을 매개한다.\n 그중에는 AMPAR의 인산화나 trafficking 같이 시냅스의 효율을 강화시키는 과정이 포함된다.\n"},"생명과학/Neuroscience/Morphological-Change":{"title":"Morphological Change","links":[],"tags":["작성중"],"content":"\nsynapse의 morphological change는 언제 일어나는가?\n여러 개의 PSD를 가진 synapse에서 F-actin과 같은 cytoskeletal filaments들의 중합으로 인해 PSD를 가진 cytoskeletons로 분리되면, 이러한 cytoskeleton에서 분비하는 adhesion molcules에 의해 두 synapse가 완전히 독립되어 새로운 spine을 형성한다[3].\n\n하지만 이러한 dendritic spine split은 LTP 및 성장 과정에서 나타나지 않는다[4].\nVs. 최근 연구에 따르면 LTP 유도에 따라 시냅스는 서로 다른 연결을 형성하는 2개의 시냅스로 나뉠 수 있다.[5]\n비교적 최근의 결과를 보는 것이 맞나?\n이전의 논문들은 대부분 특정 현상을 설명하기 위한 모델을 제시했을 뿐이다. 반면 [4]는 실험을 통해 LTP 상황에서 dendritic spine splitting이 일어나지 않음을 보였다.\nDendrite에서의 morphological change로 인해 형성된 2개의 spine이 있을 때, 각 spine에 자극이 동시에 입력된다면, dendrite의 voltage-gated channel들이 가지는 refection time으로 인해 time-dependent subtraction이 일어나지 않을까?\n그러면 morphological change와 refinement가 depression을 유도할 수 있는 건가?\ndifferentiation에서 나타나는 시냅스의 돌출부를 plarapodia라고 한다. actin filament가 자라면서 형성됨. 이로 인해 synapse migration이 일어난다.\nReferences\n[1] Lamprecht, R., LeDoux, J. Structural plasticity and memory. Nat Rev Neurosci 5, 45–54 (2004). https://doi.org/10.1038/nrn1301\n[2] ]Fiala, J., Allwardt, B. &amp; Harris, K. Dendritic spines do not split during hippocampal LTP or maturation. Nat Neurosci 5, 297–298 (2002). https://doi.org/10.1038/nn830\n[3] Davis KL,&amp;Charne y D,&amp;Coyle JT, Nemeroff CM., 2002. Neuropsychopharmacology: The Fifth Generation of Progress(5th ed.). 154\nhttps://acnp.org/digital-library/neuropsychopharmacology-5th-generation-progress/\n[4] Fiala, J., Allwardt, B. &amp; Harris, K. (2002). Dendritic spines do not split during hippocampal LTP or maturation. Nat Neurosci, 5, 297–298 https://doi.org/10.1038/nn830\n[5] Bear, Mark F., author. (2016). Neuroscience : exploring the brain. Philadelphia :Wolters Kluwer, 817\n[6] Conn, P.M. (2017), Conn’s Translational Neuroscience-Academic Press(1st ed.)"},"생명과학/Neuroscience/Neuron":{"title":"Neuron","links":["생명과학/Neuroscience/Nucleus"],"tags":[],"content":"1. Soma\n soma는 neuron의 중심부로서 원형에 가깝다. 전형적인 neuron 몸체의 직경은 약 20μm이다.\n1.1 Cytosol\n neuron의 세포막으로 외부와 격리된 K+가 풍부한 용액이다.\n1.2 Cytoplasm\n1.2.1 Organelle\n1.2.2 Nucleus"},"생명과학/Neuroscience/Nucleus":{"title":"Nucleus","links":[],"tags":["작성중"],"content":" nucleus는 구형이고, 중심부에 위치하며, 직경이 5~10μm 정도이다. 이중막인 nucleus envelope에 둘러싸여 있다. nucleus envelop에는 직경이 약 0.1μm인 nuclear pore가 존재한다. 핵안에는 chromosome이 있는데, 이는 유전물질인 DNA를 포함한다.\n1. Nucleus envelop\n1.1 Nuclear pore"},"생명과학/Neuroscience/분석-단계":{"title":"분석 단계","links":[],"tags":["작성중"],"content":"신경과학자들은 뇌 연구의 복잡성을 줄이고 체계적인 실험 분석을 위해 작은 부분들로 나누었다. 이러한 방법을 환원주의적 분석이라 부른다. 연구 단위의 크기는 흔히 분석 단계라고 한다. 복잡성의 정도에 따라 분자적, 세포적, 시스템적, 행동적, 인지적 단계로 나뉜다.\n분자신경과학\n세포신경과학\n시스템신경과학\n행동신경과학\n인지신경과학"},"생명과학/표기법":{"title":"표기법","links":[],"tags":[],"content":"유전자명은 항상 이탤릭체로 표시합니다.\n(바이러스 유전자는 소문자로 표시하고 인간 유전자는 일반적으로 모두 대문자로 표시합니다.)\n단백질의 첫글자는 대문자로 표시하며 이탤릭체로 표기하지 않습니다.\n종종 인간(사람) 유전자와 그 단백질들의 이름 앞에는 “세포성”을 의미하는 “c”를 붙인다.\n(예: C-SRC 유전자 그리고 c-Src 단백질, 또는 c-Myc 단백질)\nElement는 대부분 서열을, Factor는 단백질 의미"},"수학/Gram-Shmidt-Process":{"title":"Gram-Shmidt Process","links":[],"tags":[],"content":"주어진 벡터를 이용하여 직교 기저를 얻기 위한 과정이다.\nu1​u2​u3​uk​​=v1​=v2​−proju1​​(v2​),=v3​−proju1​​(v3​)−proju2​​(v3​),  ⋮=vk​−i=1∑k−1​projui​​(vk​),​​\n각 벡터에서 직교하는 성분만을 남겨 서로 수직하도록 한다.\nei​​=ui​/∣∣ui​∣∣​​\n위와 같이 직교 벡터들을 정규화하면, 벡터 공간의 정규 직교 기저가 된다. 즉, 주어진 벡터들이 span하는 공간의 정규 직교 기저를 얻을 수 있다."},"수학/Jacobian":{"title":"Jacobian","links":[],"tags":[],"content":"임의의 벡터 x∈Rn에 대하여 f(x)∈Rm을 만족하는 다변수 벡터 함수 f(x)가 주어졌다고 하자.\nf(x):Rn↦Rm\n이때, f의 1차 편미분은 행렬이 되고 이를 특별히 jacobian 행렬이라고 한다. 이를 통해 jacobian 행렬의 각 row vector는 함수 f에 대한 gradient라는 것을 알 수 있다. jacobian은 주로 error를 최적화 할 때 사용된다. 이때 cost funtion은 일반적으로 비선형 함수로 구성되어 있으며 크기가 작기 때문에 taylor expension하여 다음과 같은 근사식으로 표현한다.\ne(x+Δx)=e(x)+JΔx"},"수학/Logit":{"title":"Logit","links":[],"tags":[],"content":"1. Odds Ratio\nodds ratio는 특정 이벤트가 발생할 확률로 다음과 같이 쓸 수 있다.\n(1−P)P​\n여기서 P는 예측하고자 하는 대상의 확률이다. 이러한 odds ratio에 log를 취해 logit 함수를 정의한다.\n2. Logit\nlogit(P)=log1−PP​\n대상의 probability를 실수 전체에 대응 시켜 binary classification을 regression으로 접근할 수 있게 해준다.\nlogit(P(y=1∣x))=wTx\n얻고자 하는 것은 대상의 probability이므로 다음과 같이 쓸 수 있다.\nϕ(z)=1+e−z1​"},"수학/Taylor-Expansion":{"title":"Taylor Expansion","links":[],"tags":["작성중"],"content":""},"수학/Weber-Problem":{"title":"Weber Problem","links":[],"tags":["작성중"],"content":""},"영어/모르는-단어":{"title":"모르는 단어","links":[],"tags":[],"content":""},"정보과학/Algorithm/Dynamic-Time-Warping":{"title":"Dynamic Time Warping","links":[],"tags":[],"content":"DTW는 기본적으로 다이나믹 프로그래밍 기법(문제를 여러 개의 하위 문제로 분할에서 최적해를 계산하는 방법)을 이용한다. DTW를 위해 우선 두 시계열의 시점간의 유사도를 측정하기 위한 거리 함수를 정의해야 한다. 일반적으로 Euclidean distance가 많이 활용된다. DTW에서 시점을 탐색에서는 다음의 조건을 충족시켜야한다. \n\nboundary condition : Warping 거리의 첫 번째와 마지막은 이어져야 한다.\ncontinuity : Warping 경로는 대각 요소를 포함한 인접한 셀로 제한된다.\nmonotonicity: Warping 경로는 음의 방향으로 이동하지 않는다. (이미 matching된 warping이면, 이전 시점은 보지 않는다.)\nDTW는 위의 조건들을 만족하면서 Warping 거리의 합이 최소가 되는 경로를 찾는 과정이다.Warping 거리의 합은 아래 수식으로 나타난다.\n\nDTWi,j​=di,j​+min(DTWi−1,j​,DTWi,j−1​DTWi−1,j−1​)\n"},"정보과학/Algorithm/Kalman-Filter":{"title":"Kalman Filter","links":[],"tags":[],"content":"칼만 필터란 잡음이 포함된 측정치를 바탕으로 선형 역학계의 상태를 추정하는 재귀 필터이다. 칼만 필터는 예측과 업데이트의 두 단계로 이루어진다. 예측 단계에서는 현재 상태 변수의 값과 정확도를 예측한다. 현재 상태 변수의 값이 실제로 측정된 이후, 업데이트 단계에서는 이전에 추정한 상태 변수를 기반으로 예측한 측정치와 실제 측정치의 차이를 반영해 현재의 상태 변수를 업데이트한다. 이러한 칼만 필터는 선형 시스템에 대해서 동작하며 비선형 시스템을 위한 확장 칼만 필터도 존재한다. 칼만 필터는 물체의 측정값에 확률적인 오차가 포함되고, 또한 물체의 특정 시점에서의 상태가 이전 시점의 상태와 선형적인 관계를 가지고 있는 경우 적용이 가능하다.\n예를 들기 위해 우선, 공간 상태 모델 x를 다음과 같이 설정하자.\nx=(r​v​)\n여기서 r은 거리, v는 속도이다. 또한 우리가 선형 시스템을 관측함으로써 얻는 값들은 이산적이므로 샘플링 시간을 dt, k 번째 관측값을 아래 첨자 k로 표기한다.\nxk−1​=(rk−1​​vk−1​​)\nΦk​=(1dt​01​)\nxk​=Φk−1​xk−1​\nx에 노이즈가 없다면, 위와 같이 단순한 모델을 통해 상태를 예측할 수도 있다. 하지만 실제로는 측정값에 노이즈가 포함되어 있어 측정값의 진치를 알 수 없기 때문에 이러한 불확실한 측정을 통해 진짜 값을 추정할 수 밖에 없다. 칼만 필터에서는 예측 단계에서 발생하는 노이즈가 가우시안 분포를 따른다고 가정하고 노이즈 wk​를 다음과 같이 도입하였다.\nwk​∼N(0,Qk​)xk​=Φk−1​xk−1​+wk−1​​\n여기서 Qk​ 는 wk​ 의 분산이다.\n1. 상태 공분산 행렬\n칼만 필터는 추정치의 분산을 최소화하는 방향으로 동작한다. 상태 모델 x는 선형 시스템이므로 예측 오차를 아래 식과 같이 나타내며 (-)는 관측 이전의 값, (+)는 관측 이후의 값이다.\nx^k​(−)−xk​=Φk−1​(x^k−1​(+)−xk−1​)+wk−1​\n행렬 X 의 분산은 아래와 같이 오차 e의 공분산 행렬의 기댓값(E)을 통해 나타낼 수 있다.\nVar[X]=E[eeT]\n위 두 식을 통해 추정치의 사전 상태 공분산 행렬 Pk​ 를 다음과 같이 나타낼 수 있다. 추정치에 대한 값이기 때문에 (-)를 사용하여 나타내었다.\nPk​(−)=E[(x^k​(−)−xk​)(x^k​(−)−xk​)T]=E[Φk−1​(x^k​(+)−xk​)(x^k​(+)−xk​)TΦk−1T​+wk−1​wk−1T​]​\n여기서 wk​는 측정 과정에서 발생하는 무작위적인 오차이므로 계측값과 상관이 없는, 독립적인 값이므로 기댓값의 계산에는 영향을 주지 않는다. 따라서 위 식을 다음과 같이 정리할 수 있다.\nQk−1​=E[wk−1​wk−1T​]Pk​=Φk−1​Pk−1​Φk−1​+Qk−1​​\n따라서 Pk​ 는 이전 시점에서의 공분산 행렬 Pk−1​ 에 모델 Φk−1​ 을 적용한 뒤, Qk−1​ 를 더해준 형태로 나타낼 수 있다.\n2. 칼만 이득 계산\n관측 모델은 센서를 통해 측정된 신호의 실측값을 설명하기 위한 모델로 측정값 zk​를 다음과 같이 진치 xk​에 선형 변환 Hk​와 노이즈 vk​가 적용된 형태로 표현할 수 있다. 여기서 Hk​ 는 측정 신호의 왜곡을 나타낸다.\nuk​∼N(0,Rk​)zk​=Hk​xk​+uk​​\n칼만 이득은 관측치를 통해 예측치와 실측치를 각각 얼마나 반영할 것 인지를 나타내기 위한 값으로 다음과 같은 식을 통해 얻을 수 있다.\nx^k​(+)=x^k​(−)+Kk​[zk​−Hx^k​(−)]\n이때 측정기의 왜곡이 없다고 가정하면 Hk​ 는 단위 행렬이므로 다음과 같이 알파 베타 함수의 형태로 나타낼 수 있다,\nx^k​(+)=Kk​zk​+(1−Kk​)x^k​(−)\n관측 이후의 상태 공분산 행렬 Pk​ 는 다음과 같이 나타낼 수 있다.\nPk​(+)=E[eeT]=E[(x^k​(+)−xk​)(x^k​(+)−xk​)T]\n위에서 x^k​(+) 와 zk​를 구했으므로 이를 대입해보면\nPk​(+)=E[(Kk​zk​+(1−Kk​)x^k​(−)−xk​)(Kk​zk​+(1−Kk​)x^k​(−)−xk​)T]=E[(Kk​Hk​xk​+Kk​vk​+(1−Kk​)x^k​(−)−xk​)(Kk​Hk​xk​+Kk​vk​+(1−Kk​)x^k​(−)−xk​)T]=E[[Kk−1​(xk​−x^k​(−))+Kk​vk​][Kk−1​(xk​−x^k​(−))+Kk​vk​]T]=Pk​(−)−Kk​Hk​Pk​(−)−Pk​(−)HkT​KkT​+Kk​(HPk​(−)HT+Rk​)KkT​​\n오차 공분산 행렬 Pk​의 크기를 최소화하는 칼만 이득 Kk​ 를 추정하기 위해서는 tr(Pk​)를 Kk​에 대해 미분하여 0이 되는 지점을 찾으면 된다.\ndKk​dtr(Pk​)​=−2(Hk​Pk​(−))T+2Kk​(Hk​Pk​(−)HkT​+Rk​)=0\n따라서 Kk​는 다음과 같이 쓸 수 있다.\nKk​=Pk​(−)+Rk​Pk​(−)​\n일반적으로 예측 모델의 성능이 더 높기 때문에 Rk​&gt;Qk​ 인 경우가 많다. Rk​가 크다는 것은 관측 모델의 오차가 예측 모델에 비해 크다는 것을 의미하기 때문에 칼만 이득이 작아지고 사후 추정치 x^k​(+) 를 구할 때 추정치 x^k​(−) 의 영향이 커지게 된다."},"정보과학/Anaconda":{"title":"Anaconda","links":[],"tags":[],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncommand기능conda create -n 가상환경이름 python=버전가상환경 생성conda env remove —n 가상환경이름가상환경 제거conda activate 가상환경이름가상환경 실행jupyter notebook주피터 노트북 실행"},"정보과학/Data-Structure/Binary-Indexed-Tree":{"title":"Binary Indexed Tree","links":["정보과학/실습/Binary_Indexed_Tree.py"],"tags":[],"content":"이진 인덱스 트리(Binary Indexed tree, 이하 BIT)는 prefix sum에서 값을 변경할 때 해당 값을 기준으로 이후의 값들을 전부 다시 계산해야 한다는 단점을 해결해 준다.\n1. 트리 구성\n1.1. 구간의 크기\n이러한 BIT에서는 인덱스를 이진수로 보았을 때, 가장 작은 자릿수를 가지는 1의 위치에 따라 구간의 크기를 정한다. 예를 들어, BIT가 저장된 배열 T에서 12번 노드는 이진수로 1100이다. 이때 1100에서 가장 작은 자릿수를 가지는 1은 4의 자리에 있다.  따라서 12번 노드의 구간은 12-4+1 ~ 12이다. (구간의 크기가 4)\n1.2. 가장 작은 1의 위치\n위에서 구간의 크기를 정하기 위해 가장 작은 자릿수를 가지는 1의 위치를 사용했다. 이는 다음과 같이 보수와의 bitwise AND를 통해 구할 수 있다.\nk=10100...00−k=01011...11+0001=01100...00k&amp;−k=00100...00​\n이런 과정을 거치면 1이 몇 번째에 있든 상수 시간 내에 찾을 수 있다.\nT[k]=P[k]−P[k−dk+1](dk=k&amp;−k,P=prefixsum)\n2. 구간 합\n2.1. prefix sum 구하기\nT[k]=P[k]−P[k−dk+1]T[k−dk]=P[k−dk]−P[k−dk−dk+1]​\n따라서 T[k]는 T[k−d]와 구간이 겹치지 않는다. 이를 통해 아래와 같이 P[k]를 구할 수 있다.\nP[k]=T[k]+T[k−dk]+T[k−dk−dk−dk]...+T[1]+T[0]\n2.2. 구간 합  구하기\n위와 같이 BIT를 통해 prefix sum을 구한 뒤 이를 통해 원하는 구간 합 S를 구한다.\nS[i,j]=P[j]−P[i−1]\n3. 업데이트\n3.1. 부모 노드 찾기\nBIT에서 T[k]는 구간 k−d+1∼k를 가진다. 이를 통해 부모 노드를 찾아보자.\n\n위 그림을 보면, 노드 k의 부모가 k+d이다. 구간 k, k+d가 아래와 같다고 하자.\nk−dk≤x≤kk+dk−dk+dk≤x≤k+dk​\n이때 dk≤dk+dk−dk이므로 구간 k+d는 구간 k를 포함한다. 이렇게 k+d가 항상 k가 가지는 구간을 포함하기 때문에 노드 k의 부모는 노드 k+d이다.\n3.2 값 변경\n어떤 노드의 값을 변경하면 BIT에서는  해당 노드를 포함하는 구간만 업데이트하면 된다. 위 과정을 통해 부모 노드의 인덱스가 배열의 길이를 벗어날 때까지 부모 노드에 T[k]의 값이 변한 만큼 더해주면 된다.\nd=T[k]−T[k]′T[k+dk]=T[k+dk]+dT[k+dk+dk+dk]=T[k+dk+dk+dk]+d​\n4. 코드\nTransclude of Binary_Indexed_Tree.py"},"정보과학/Data-Structure/k-d-Tree":{"title":"k-d Tree","links":[],"tags":["작성중"],"content":""},"정보과학/Dimension-Reduction/Linear-Discriminant-Analysis":{"title":"Linear Discriminant Analysis","links":[],"tags":["작성중"],"content":"전체 데이터를 특정 직선에 projection하였을 때 같은 class 내의 데이터 사이의 거리를 가깝게 하는 벡터를 찾는다. 즉, 각 클래스의 covariance의 합이 최대한 작게, 각 class간의 거리는 최대한 크게 하는 벡터 W를 찾는다.\nwTΣ0​w+wTΣ1​w∣∣wTμ0​−wTμ1​∣∣​\n각 클래스의 covariance의 합과 class 간 거리는 위와 같이 쓸 수 있다. 따라서 cost function은 다음과 같이 쓸 수 있다.\nJ=∣∣wTμ0​−wTμ1​∣∣wTΣ0​w+wTΣ1​w​​\nSw​와 Sb​는 각각 집단 내 산포 행렬과 집단 간 산포 행렬로 대칭행렬이며 다음과 같이 표현된다.\nSw​=Σ1​+Σ2​Σi​=x∈χi​∑​(x−μi​)(x−μi​)TSb​=(μ0​−μ1​)(μ0​−μ1​)TJ=wTSw​wwTSb​w​​\n최대화를 위해 목적함수 J(w)를 미분하여 0이 되는 지점을 찾으면 다음과 같은 식을 얻을 수 있다.\n이를 목적함수 J로 나타내면\n이를 통해 우리가 찾고자 하는 벡터 w는  의 고유벡터임을 알 수 있다. 따라서 해당 고유벡터를 찾는다면 차원축소를 위해 찾고자 했던 Projection vector w를 찾을 수 있다.\nhttps://direction-f.tistory.com/82"},"정보과학/Dimension-Reduction/Principal-Component-Analysis":{"title":"Principal Component Analysis","links":[],"tags":["작성중"],"content":""},"정보과학/Dimension-Reduction/t-distributed-Stochastic-Neighbor-Embedding":{"title":"t-distributed Stochastic Neighbor Embedding","links":[],"tags":["작성중"],"content":""},"정보과학/Machine-Learning/Architecture/Attention":{"title":"Attention","links":[],"tags":[],"content":"Attention\n입력된 query에 대해서 내적과 같은 연산을 통해 key와의 유사도를 측정하고 내적 값의 합이 1이 되도록 내적 값의 합으로 나눠주어 정규화한다.\nSelf-Attention\nattention에 query와 key 모두 입력된 문장으로 사용한다고 할 때 각각의 단어에 대한 가중치 합을 구하면 이는 자기 자신과 문장 내에 포함된 다른 단어들로 이루어진 정보가 포함되어 있다. 이를 신경망에 입력해 분석하면 자기 자신 뿐만 아니라 주변 단어들과의 연관성을 통해 간접적으로 나마 단어의 의미를 분석할 수 있다.\nMulti-Head Attention\nattention을 수행할 때 query, key, value를 head의 수 만큼 분리한 뒤 각각에 대해 독립적으로 attention을 수행한다. 이때 각 head는 주어진 subsequence 내에서 attention을 주기 때문에 모델이 input token에서 다양한 종속 관계를 표현할 수 있게 된다. 이후 attention value를 concatenate하여 attention을 수행했을 때와 동일한 차원의 결과를 얻을 수 있다. 즉, 병렬 연산을 수행하기 위해 여러 head로 나눈 뒤 결과를 합치는 것\nScaled Dot-Product Attention\n기존의 dot-product attention은 embedding vector의 차원 dk​가 커질수록 값이 커질 가능성이 높아진다. 이로 인해 softmax에서 gradient가 작은 구간에 쉽게 도달하는데, 이를 해결하고자 dk​​를 통해 scailing 해준 것이다."},"정보과학/Machine-Learning/Architecture/Convolutional-Layer":{"title":"Convolutional Layer","links":["정보과학/Machine-Learning/Architecture/Pooling","정보과학/Machine-Learning/Architecture/Fully-Connected-Layer"],"tags":[],"content":"일반적으로 CNN은 여러 개의 convolution layer와 Pooling layer라고도 하는 subsampling layer로 이루어져 있다.  마지막에는 하나 이상의 Fully Connected Layer가 있다.\nCNN은 이미지와 관련된 작업을 매우 잘 수행하는데, 이는 대체적으로 다음 두 개의 아이디어 때문이다.\n\n희소 연결: feature map에 있는 하나의 원소는 작은 pixel patch 하나에만 연결된다.\nparameter 공유: 동일한 가중치가 입력 이미지의 모든 patch에 사용된다.\n\n1. Discrete Convolution\n두 개의 벡터 x, w에 대한 discrete convolution은 y=x∗w와 같이 나타낸다. 여기서 x는 입력, w는 kernel이다. 1차원에서 discrete convolution의 정의는 다음과 같다.\ny=x∗w→y[i]=k=0∑m−1​x[i+m−k]w[k]\n여기서 x와 w의 indexing이 반대 방향이기 때문에 입력 또는 kernel을 뒤집어 내적을 수행한다. 이를 2차원으로 확장하면 다음과 같이 표현할 수 있다.\nY=X∗W→Y[i, j]=k1​=0∑m1​−1​k2​=0∑m2​−1​X[i+m1​−k1​, i+m2​−k2​]W[k1​,k2​]\n1.1. 1x1 Convolution\n각 pixel에 대해 곱셈을 수행한 뒤 모두 더하기 때문에, 채널 간의 weighted sum과 같다.\n2. Kernel\nkernel의 역할은 이미지로부터 내가 원하는 특징만을 추출하는 것이다. kernel은 이미지 데이터를 지정된 간격(stride)으로 움직이며 convolution을 수행하고 convolution을 통해 나온 결과가 feature map이다. 일반적으로 (2,2), (3,3) 등의 정방 행렬로 정의되고 CNN에서 학습의 대상은 필터의 파라미터이다. 입력 데이터의 각각의 채널에서 지정된 간격으로 순회하며 convolution을 수행하고 모든 채널의 convolution의 합인 feature map이 출력으로 나온다.\n3. Pooling\nfeature map의 parameter를 줄이기 위해 사용되며 overfitting을 억제한다."},"정보과학/Machine-Learning/Architecture/Encoder-and-Decoder":{"title":"Encoder&Decoder","links":["정보과학/Machine-Learning/Learning-Strategy/Teacher-Forcing"],"tags":[],"content":"Encoder는 문장 내에서 정보를 추출하여 Context Vector 형식으로 저장한다.\nDecoder는 이러한 Context Vector를 다시 문장으로 바꾸는 역할을 한다.\n만약 만들고자 하는 모델이 번역기라면,\nEncoder는 한국어 문장을 Context Vector 형식으로 변환할 것이고\nDecoder에서는 Context Vector를 통해 영어 문장을 생성할 것이다.\n즉, Context Vector는 문장의 의미, 단어의 위치 등이 저장된 정보 전달 수단이다.\nEncoder\nDecoder\nTeacher Forcing\ndecoder는 이전 예측을 바탕으로 다음 단어들을 예측해나가기 때문에 예측이 정답과 조금만 달라져도 전혀 다른 문장을 만들어낼 수 있다. 이를 해결하기 위해 정답 데이터를 이전 예측이라고 생각하여 학습시킨다."},"정보과학/Machine-Learning/Architecture/Feed-Forward-Network":{"title":"Feed-Forward Network","links":[],"tags":[],"content":"Position-Wise FFN\nposition-wise FFN은 3개의 layer와 1개의 activation function으로 이루어져 있다. hidden layer와 model input의 차원이 다르기 때문에 두 개의 선형 변환 W1​=(dmodel​,dffn​), W2​=(dffn​,dmodel​)로 표현되며 다음과 같이 쓸 수 있다.\nσ:activation functionFFN(x)=σ(xW1​+b1​)W2​+b2​​"},"정보과학/Machine-Learning/Architecture/Flatten-Layer":{"title":"Flatten Layer","links":["정보과학/Machine-Learning/Architecture/Convolutional-Layer","정보과학/Machine-Learning/Architecture/Fully-Connected-Layer"],"tags":[],"content":"Convolutional Layer에서 사용되는 feature map 등의 2D, 3D 데이터를 하여 Fully Connected Layer 등에서 다룰 수 있도록 1D vector로 변환한다."},"정보과학/Machine-Learning/Architecture/Fully-Connected-Layer":{"title":"Fully Connected Layer","links":[],"tags":["작성중"],"content":""},"정보과학/Machine-Learning/Architecture/Global-Average-Pooling":{"title":"Global Average Pooling","links":[],"tags":["작성중"],"content":"이미지의 feature map에 담긴 정보를 압축하고 parameter를 줄이기 위해 각 feature map의 average를 구해 feature map들을 1D vector로 변환한다."},"정보과학/Machine-Learning/Architecture/Logistic-Regression":{"title":"Logistic Regression","links":["정보과학/실습/Logistic_Regression.ipynb"],"tags":["작성중"],"content":"logistic regression은 구현하기 매우 쉽고 선형적으로 구분되는 class에 뛰어난 성능을 내는 분류 모델이다.\nTransclude of Logistic_Regression.ipynb"},"정보과학/Machine-Learning/Architecture/Pooling":{"title":"Pooling","links":["정보과학/Machine-Learning/Architecture/Convolutional-Layer"],"tags":[],"content":"Max Pooling\nkernel에 포함되는 영역 안에서 최대 값을 추출한다. 일반적으로 Convolutional Layer의 feature map에서는 보고자 하는 feature의 값이 크게 형성되기 때문에 max pooling을 주로 사용한다.\nAverage Pooling\nkernel에 포함되는 영역의 평균 값을 추출한다. 과거 LeNet 등에서 사용되었으나 이미지를 전체적으로 smoothing 시키는 경향이 있어 sharp feature가 소실되는 문제가 있다. 조금 다른 방향으로 생각해보면 CNN에서 average pooling의 경우 kernel을 통해 구현이 가능하다(kernel내의 weight가 동일하다면 average pooling과 같은 역할을 수행할 것이다).\nMean Pooling\nkernel에 포함되는 영역 안에서 최소 값을 추출한다."},"정보과학/Machine-Learning/Architecture/Skip-Connection":{"title":"Skip Connection","links":["정보과학/Machine-Learning/Learning-Strategy/Ensemble","정보과학/Machine-Learning/Learning-Strategy/Regularization"],"tags":[],"content":"일반적으로 CNN에서는 필터들이 convolution을 수행하기 때문에 layer가 깊어질수록 채널의 수가 많아지고 이미지의 너비와 높이가 줄어든다. VGGNet은 이러한 문제점을 해결하기 위해 3x3 필터를 사용하여 layer의 깊이를 늘렸다. 하지만 parameter의 개수가 매우 많았고 layer가 깊어질수록 오차율이 증가하는 문제가 있어 이를 해결하기 위한 방법으로 residual learning가 제안 되었다[1].\nSkip Connection\n실제로 학습 시키고자 하는 mapping을 H(x)라고 하면, 이를 학습 시키는 것은 어려우므로 residual block을 통해 H(x)=F(x)−x의 형태로 변형하여 학습에 용이한 F(x)를 사용한다. skip connection 사이에는 여러 개의 activation function이 존재할 수 있으며 convolution으로 인한 차원 감소는 Ws​를 이용한 linear projection을 통해 해결한다.\nF(x)=H(x)+x=F(x,{Wi​})+Ws​x\n해석\n원 논문에서는 residual block에 의한 결과를 경험적으로 얻었다고 말하였다[1]. 이에 따라 ResNet이 왜 잘 작동하는지 설명하려는 많은 노력이 있었다.\n1. Ensemble\n[2]에서는 unraveled의 관점에서 본다면 residual networks는 Droupout처럼 다른 길이의 많은 paths의 모음으로 볼 수 있다. residual networks의 paths가 서로 의존하는지 또는 어느 정도 중복성을 나타내는 지에 대한 의문을 제기할 수 있다. 일반적으로 neural network가 model structure의 급격한 변화에 견딜 수 있는지는 분명하지 않지만, layer를 삭제하면 모든 후속 layer의 input distribution이 크게 변경되기 때문에 깨질 것으로 예상한다. [2]에서 실험한 lesion study에서 paths가 각 다른 것에 강력하게 종속되어있지 않으며, ensemble 같은 행동을 보인다는 것을 밝힌다.\n2. Optimal Depth\n문제 해결에 있어서 가장 optimal한 depth를 찾는 방법은 아직 없다. 하지만 skip connection이 존재한다면, optimal depth 이후의 weight와 bias가 모두 0으로 수렴하여 optimal depth에서의 output이 바로 classification으로 넘어갈 수 있다.\nReferences\n[1] He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n[2] Veit, Andreas, Michael Wilber, and Serge Belongie. “Residual networks behave like ensembles of relatively shallow networks.” arXiv preprint arXiv:1605.06431 (2016)."},"정보과학/Machine-Learning/Architecture/Spatial-Pyramid-Pooling":{"title":"Spatial Pyramid Pooling","links":["정보과학/Machine-Learning/Architecture/Flatten-Layer","정보과학/Machine-Learning/Architecture/Fully-Connected-Layer"],"tags":[],"content":"Flatten Layer의 경우 feature map을 1D vector로 변환하는 역할만 수행하기 때문에 이미지의 크기에 따라 vector의 크기가 변해 Fully Connected Layer를 사용하지 못한다는 단점이 있었다. 이를 해결하기 위해 서로 다른 크기의 kernel을 통해 pooling을 수행한 뒤 이를 flatten&amp;concat 하여 고정된 크기의 vector를 만들어낸다. kernel의 크기가 고정되어 있지 않기 때문에 다양한 크기의 feature를 찾아낼 수 있다."},"정보과학/Machine-Learning/Auto-Differentiation":{"title":"Auto Differentiation","links":[],"tags":["작성중"],"content":"https://velog.io/@gypsi12/Auto-Differentiation%EC%9E%90%EB%8F%99%EB%AF%B8%EB%B6%84-%EC%9D%B4%EB%9E%80"},"정보과학/Machine-Learning/Computer-Vision/Bounding-Box-Regression":{"title":"Bounding Box Regression","links":[],"tags":[],"content":"region proposal을 통해 예측한 bounding box(P)를 ground truth(G)에 맞추기 위하여 사용된다. regression을 통해 예측한 bounding box를 P′이라고 할 때 ground truth에 대한 보정 값 d(P′)과 같이 나타내면 아래와 같이 쓸 수 있다.\nG^x​G^y​G^w​G^h​​=Pw′​dx​(P′)+Px′​=Ph′​dy​(P′)+Py′​=Pw′​exp(dw​(P′))=Ph′​exp(dh​(P′))​\nregression의 목적은 region proposal한 bounding box를 ground truth로 보정하는 것이므로 target은 다음과 같이 쓸 수 있다.\ntx​ty​tw​th​​=Pw​Gx​−Px​​=Ph​Gy​−Py​​=log(Pw​G^w​​)=log(Ph​G^h​​)​​\n따라서 loss function(J)는 다음과 같다.\ni∈x,y,w,h∑​(ti​−di​(P′))2+λ∣∣w∣∣2​​"},"정보과학/Machine-Learning/Computer-Vision/Data-Augmentation":{"title":"Data Augmentation","links":[],"tags":["작성중"],"content":""},"정보과학/Machine-Learning/Computer-Vision/Evaluation":{"title":"Evaluation","links":[],"tags":["작성중"],"content":"Intersection of Union\nNon Max Supression\nmAP"},"정보과학/Machine-Learning/Computer-Vision/Non-Maximum-Suppression":{"title":"Non-Maximum Suppression","links":[],"tags":[],"content":"detect된 bounding box 중에 비슷한 위치에 있는 box를 제거하고, 가장 적합한 bbox를 선택한다.\n\n모든 bounding box에 대하여 threshold 이하의 confidence score를 가지는 bounding box는 제거한다.\n남은 bounding box들을 confidence score 기준 모두 내림차순 정렬한다.\n맨 앞에 있는 bounding box 하나를 기준으로 잡고, 다른 bounding box와 IoU 값을 구한다. IoU가 threshold 이상인 bounding box들은 제거한다.\n해당 과정을 순차적으로 시행하여 모든 bounding box를 비교하고 제거한다.\n"},"정보과학/Machine-Learning/Computer-Vision/Region-Proposal/Selective-Search":{"title":"Selective Search","links":[],"tags":["작성중"],"content":"빠른 detection과 높은 recall을 동시에 만족하는 방식으로 색, 텍스쳐, 크기, 형태 등에 따라 유사한 region을 계층적으로 그룹핑한다.\n\nsegment된 모든 부분들을 bounding box로 만들어서 region proposal 리스트에 추가한다.\n색, 텍스쳐, 크기, 형태 등에 유사한 segment들을 그룹핑한다.\n"},"정보과학/Machine-Learning/Computer-Vision/Region-Proposal/Sliding-Window":{"title":"Sliding Window","links":[],"tags":["작성중"],"content":"Window를 왼쪽 상단에서 부터 오른쪽 하단으로 이동시키면서 object를 detection하는 방식\n\n다양한 형태의 window를 각각 sliding 시키는 방식\nwindow scale은 고정하고 scale을 변경한 여러 이미지를 사용하는 방식\n"},"정보과학/Machine-Learning/Computer-Vision/model/AlexNet":{"title":"AlexNet","links":["정보과학/Machine-Learning/Architecture/Convolutional-Layer","정보과학/Machine-Learning/Architecture/Pooling","정보과학/Machine-Learning/Learning-Strategy/Normalization","정보과학/Machine-Learning/Architecture/Flatten-Layer","정보과학/Machine-Learning/Architecture/Fully-Connected-Layer","Softmax","정보과학/Machine-Learning/Computer-Vision/Data-Augmentation","정보과학/Machine-Learning/Learning-Strategy/Regularization","정보과학/Machine-Learning/Learning-Strategy/Optimizer"],"tags":[],"content":"\n1. Architecture\n1.1. Convolutional Layer\n두 개의 GPU에서 독립적으로 학습시킴으로써 서로 다른 feature map을 추출하는 layer를 구성하였으며, 3번째 layer에서 convolution 결과를 concat 해주었으며 해당 layer에서는 convolution만 수행하였다.\n1.1.1. ReLU\n1.1.2. Max Pooling\n1.1.3. Local Response Normalization\n1.2. Flatten Layer\n1.3. Fully Connected Layer\n1.4. Softmax\n2.Training\n2.1. Data Augmentation\n2.2. Regularization\n2.2.1. Dropout\n2.2.2. Weight Decay\n2.3. Stochastic Gradient Descent\n2.3.1. Momentum"},"정보과학/Machine-Learning/Computer-Vision/model/GoogLeNet":{"title":"GoogLeNet","links":[],"tags":["작성중"],"content":""},"정보과학/Machine-Learning/Computer-Vision/model/R-CNN/Fast-R-CNN":{"title":"Fast R-CNN","links":["정보과학/Machine-Learning/Computer-Vision/model/R-CNN/R-CNN","정보과학/Machine-Learning/Architecture/Fully-Connected-Layer","정보과학/Machine-Learning/Computer-Vision/Region-Proposal/Selective-Search","정보과학/Machine-Learning/Computer-Vision/model/VGGNet","정보과학/Machine-Learning/Computer-Vision/Bounding-Box-Regression"],"tags":[],"content":"R-CNN에서 고정된 크기의 이미지를 받았던 것은 bbox regression 및 confidence score를 얻기 위해 사용되는 Fully Connected Layer가 고정된 크기의 input을 요구하기 때문인데, 이 과정에서 이미지를 warp하게 되면 이미지의 비율이 깨지는 등 원본의 정보가 소실된다. 또한 R-CNN의 bbox regression과 SVM에서 각각 CNN 전후의 값을 사용하기 때문에 학습 과정이 구분되었으나 Fast R-CNN에서는 RoI pooling을 도입함으로써 두 과정 모두 RoI의 feature vector를 사용하기 때문에 End-to-End 학습이 가능해졌다.\n1. Architecture\n1.1. Region Proposal\nR-CNN과 마찬가지로 Selective Search를 통해 region proposal을 추출한다.\n1.2. Backbone\n1.2.1. VGGNet\n1.3. RoI Pooling\nfeature map에서 RoI에 해당하는 부분을 max pooling을 통하여 고정된 길이의 feature vector로 축소한다.\n1.4. Head\n1.4.1. Fully Connected Layer\n1.4.1.1. Softmax\nfeature vector를 통해 classification을 수행한다.\n1.4.1.2. Bounding Box Regression"},"정보과학/Machine-Learning/Computer-Vision/model/R-CNN/Faster-R-CNN":{"title":"Faster R-CNN","links":["정보과학/Machine-Learning/Computer-Vision/model/R-CNN/Fast-R-CNN","정보과학/Machine-Learning/Computer-Vision/Region-Proposal/Selective-Search","정보과학/Machine-Learning/Architecture/Convolutional-Layer","정보과학/Machine-Learning/Computer-Vision/model/VGGNet","정보과학/Machine-Learning/Computer-Vision/Bounding-Box-Regression","정보과학/Machine-Learning/Computer-Vision/Non-Maximum-Suppression"],"tags":[],"content":"기존의 Fast R-CNN은 region proposal 과정에서 CPU 기반의 Selective Search와 같은 방식을 사용하였기 때문에 bottleneck으로 작용하였으나 Faster R-CNN에서는 Region Proposal Network를 GPU에서 연산함으로써 해결하였다.\n1. Architecture\n1.1. Convolutional Layer\n1.1.1 VGGNet\n600×1000×3 이미지를 입력 받아 W×H×512 conv feature map을 출력한다.\n1.2. Region Proposal Network\nFast-RCNN에서는 Region proposal과 CNN을 통한 feature map 추출이 별도로 진행되었으나 이러한 conv feature map을 region proposal에 사용할 수 있음을 확인하였다. 따라서 몇 개의 convolutional layer를 추가함으로써 region proposal, Bounding Box Regression 그리고 classification에 대한 scoring을 동시에 수행하도록 하였다.\n1.2.1. Anchor\n고정된 크기의 bounding box를 사용하는 경우 다양한 크기의 object를 인식하지 못하는 문제가 발생할 수 있다. 따라서 사전에 정의된 다양한 scale 및 aspect ratio를 만족하는 anchor box를 생성한다. aspect ratio가 1:2인 경우 anchor box의 크기는 다음과 같이 구할 수 있다.\nW×H2WWH​=scale=s2=H=2​s​=2​s​​\n이때 k는 가능한 anchor box의 수로 scale×aspect ratio이다. VGG-16에서 출력된conv feature map의 사이즈가 W×H×512이므로 anchor box는 W×H×k개가 생성된다.\nNon-Maximum Suppression 과정에서 높은 IoU를 갖는 anchor box에 할당한다.\n1.2.1.2. Box-Classification Layer\n9×2개의 1×1 kernel로 구성된 layer로, 해당 anchor box의 각 픽셀에 object가 존재할 확률을 예측한 뒤 softmax 등을 통해 anchor box 내에 class가 존재할 확률을 추정한다. W×H×k개의 anchor box에 대해 prediction하므로  (W×H×k)×2 dimension vector를 생성하며 logistic regression을 사용하여도 무방하다. 이후 class가 존재할 확률이 높은 anchor box에 대하여 bounding box regression이 이루어진다.\n1.2.1.1. Box-Regression Layer\n9×4개의 1×1 kernel로 구성된 layer로, 해당 anchor box를 통해 bounding box의 tx​, ty​, tw​, th​를 predict한다. 즉, 이후에 있을 bounding box regression을 위한 RoI를 제공한다.\n1.3. Fast R-CNN\nRoI Pooling을 통해 전달 받은 RPN에 의해 predict된  bounding box와 conv feature map을 활용하여 classification 및 bbox regression을 수행한다.\n2. Training\nReference\n[1] Ren, Shaoqing, et al. “Faster r-cnn: Towards real-time object detection with region proposal networks.” Advances in neural information processing systems 28 (2015)."},"정보과학/Machine-Learning/Computer-Vision/model/R-CNN/R-CNN":{"title":"R-CNN","links":["정보과학/Machine-Learning/Computer-Vision/Region-Proposal/Selective-Search","정보과학/Machine-Learning/Computer-Vision/model/AlexNet","정보과학/Machine-Learning/Computer-Vision/Evaluation","정보과학/Machine-Learning/Learning-Strategy/Batch","정보과학/Machine-Learning/Learning-Strategy/Hard-Negative-Mining","정보과학/Machine-Learning/model/Support-Vector-Machine","정보과학/Machine-Learning/Computer-Vision/Bounding-Box-Regression"],"tags":[],"content":"1. Architecture\n1.1. Region Proposal\nSelective Search를 통해 약 2000개의 region proposal을 먼저 추출한 후 CNN 모델에 들어간다. 이때 pre-trained model을 사용하므로 모델의 입력 이미지 크기에 맞추어 resize한다.\n1.2. Backbone\nresize된 region proposal을 입력해 feature vector를 추출한다. (2000개의 4096 sized feature vector)\n1.2.1. AlexNet\nselective search를 사용한 region proposal에는 객체 뿐만 아니라 배경이 포함될 수 있기 때문에 selective search를 통해 배경을 포함한 학습 데이터를 제작한 뒤 ground truth와의 IoU를 구한다. IoU 값이 0.5 이상인 경우 positive sample, 이하인 경우 negative sample로 지정한다. 그리고 positive saple 32개, negative sample 96개로 구성된 Mini-Batch를 구성하여 fine-tuning한다. 이후 Hard Negative Mining을 통해 다시 한 번 학습시킨다.\n1.3. Head\n1.3.1. Support Vector Machine\nCNN에서 추출된 feature vector가 분류된 class에 해당하는지 판별하는 이진 분류기로 confidence score를 얻기 위해 사용된다.\n1.3.2. Bounding Box Regression"},"정보과학/Machine-Learning/Computer-Vision/model/ResNet":{"title":"ResNet","links":["정보과학/Machine-Learning/Architecture/Skip-Connection","정보과학/Machine-Learning/Architecture/Fully-Connected-Layer"],"tags":["작성중"],"content":"DNN에서의 vanishing gradient 및 degradation problem을 해결하기 위한 방안으로 Skip Connection을 제시하였다.\n\n1. Architecture\n1.1. Fully Connected Layer\n1.1.1. ReLU\n1.2. Skip Connection\nReferences\n[1] He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."},"정보과학/Machine-Learning/Computer-Vision/model/SPPnet":{"title":"SPPnet","links":["정보과학/Machine-Learning/Computer-Vision/model/R-CNN/R-CNN","정보과학/Machine-Learning/Architecture/Fully-Connected-Layer","정보과학/Machine-Learning/Computer-Vision/Region-Proposal/Selective-Search","정보과학/Machine-Learning/Architecture/Convolutional-Layer","정보과학/Machine-Learning/Architecture/Spatial-Pyramid-Pooling","정보과학/Machine-Learning/model/Support-Vector-Machine","정보과학/Machine-Learning/Computer-Vision/Bounding-Box-Regression","정보과학/Machine-Learning/Computer-Vision/Non-Maximum-Suppression"],"tags":[],"content":"R-CNN에서는 Fully Connected Layer가 고정된 크기의 vector만을 입력 받기 때문에 입력 이미지의 크기가 매우 제한적이었으나, 이미지의 크기에 무관하게 고정된 크기의 vector를 생성할 수 있는 방식을 제안하였다.\n1. Architecture\n1.1. Region Proposal\nSelective Search를 통해 약 2000개의 RoI 후보군을 추출한다.\n1.2. Convolutional Layer\n이미지에 crop, warp를 수행하지 않고 convolutional layer에 입력한다. 이때 feature map에 2000개의 RoI를 projection하여 적용한다.\n1.3. Spatial Pyramid Pooling\n1.4. Support Vector Machine\n1.5. Bounding Box Regression\n1.6. Non-Maximum Suppression"},"정보과학/Machine-Learning/Computer-Vision/model/VGGNet":{"title":"VGGNet","links":["정보과학/Machine-Learning/Architecture/Convolutional-Layer","정보과학/Machine-Learning/Architecture/Pooling","정보과학/Machine-Learning/Architecture/Flatten-Layer","정보과학/Machine-Learning/Architecture/Fully-Connected-Layer","Softmax"],"tags":[],"content":"\n1. Architecture\n1.1. Convolutional Layer\n1.1.2. Max Pooling Layer\n1.3. Flatten Layer\n1.4. Fully Connected Layer\n1.5. Softmax"},"정보과학/Machine-Learning/Computer-Vision/model/ViT":{"title":"ViT","links":["정보과학/Machine-Learning/Architecture/Attention","정보과학/Machine-Learning/Method/Positional-Encoding","정보과학/Machine-Learning/Learning-Strategy/Normalization"],"tags":[],"content":"1. Architecture\n1.1. Encoder\n\n이미지를 고정된 크기의 patch로 나누어 준 뒤 각 patch를 flatten하고 linear projection하여 1D vector를 얻는다.\n1.1.1. Class Token\nclassification을 수행하기 위해 전체 patch에 대한 insight를 얻기 위해 추가되었다. Multi-Head Attention 과정을 거치며 이미지의 patch를 통해 표현되기 때문에 MLP에 전달되어 classification에 사용된다. 구현 과정에서는 normal distribution을 따르는 임의의 vector로 생성된다.\n1.1.2. Positional Encoding\n위치 정보를 보존하기 위해 positional encoding을 더해주며 이미지임에도 1D positional encoding을 사용했을 때 더 나은 성능을 보였다.\n1.1.3. Layer Normalization\n1.1.4. Multi-Head Attention"},"정보과학/Machine-Learning/Computer-Vision/model/YOLO/YOLOv1":{"title":"YOLOv1","links":["정보과학/Machine-Learning/Computer-Vision/Bounding-Box-Regression","정보과학/Machine-Learning/Computer-Vision/model/GoogLeNet","정보과학/Machine-Learning/Architecture/Convolutional-Layer","정보과학/Machine-Learning/Architecture/Fully-Connected-Layer"],"tags":[],"content":"기존의 object detection model은 Bounding Box Regression 및 classification으로 이루어진 2 stage detector이며 pipeline이 복잡해 학습과 예측 과정이 느리다. VOLO v1은 하나의 CNN을 통해 bounding box와 class를 예측하는 1 stage detector로 속도가 빠르다.\n1. Architecture\n\n1.1. GoogLeNet\n1.2. Convolutional Layer\n1.3. Fully Connected Layer\n1.4. Prediction Tensor\n7×7  grid cell 내의 점을 중심으로 갖는 2개의 bounding box의 (x,y,w,h,confidence)와 ImageNet이 포함하는 20 class에 대한 prediction을 포함한다. 여기서 confidence score는 grid cell 내에 class가 존재하는지(0 or 1)와 ground truth와의 IoU를 곱해 계산한다."},"정보과학/Machine-Learning/Computer-Vision/model/YOLO/YOLOv2":{"title":"YOLOv2","links":["정보과학/Machine-Learning/Computer-Vision/model/YOLO/YOLOv1","정보과학/Machine-Learning/Learning-Strategy/Normalization","정보과학/Machine-Learning/Architecture/Convolutional-Layer","정보과학/Machine-Learning/Architecture/Fully-Connected-Layer","정보과학/Machine-Learning/Computer-Vision/model/R-CNN/Faster-R-CNN","K-means-Clustering","정보과학/Machine-Learning/Computer-Vision/Bounding-Box-Regression","정보과학/Machine-Learning/Architecture/Global-Average-Pooling"],"tags":[],"content":"1. Architecture\n논문에서는 기존의 YOLOv1의 성능을 향상시키기 위한 메소드를 정확도(better), 속도(faster), class 개수(stronger) 등으로 나눠 소개하였기 때문에 이를 따라 정리하였다.\n1.1. Better\n1.1.1. Batch Normalization\n모든 Convolutional Layer에 batch normalization을 추가함으로써 이전 layer의 parameter 변화로 인해 데이터의 입력 분포가 바뀌는 현상을 방지하였다.\n1.1.2. High Resolution Classifier\nfeature map을 추출할 때 사용하는 backbone 모델은 보통 classification을 수행하기 때문에 low resolution 이미지를 사용하여도 되었으나 object detection을 위해서는 high resolution 이미지를 사용하는 것이 적합하기 때문에 224×244의 이미지를 사용하던 네트워크를 448×448로 fine tuning하였다.\n1.1.3. Convolutional With Anchor Box\n\nFully Connected Layer를 Convolutional Layer로 대체하였으며 원본 이미지를 S×S의 grid로 나눈 뒤 각 grid cell별로 5개의 anchor box를 예측한다. output tensor는 각 anchor box에 대한 (x,y,w,h)와 각 class에 대한 probability를 포함한다.\n1.1.4. Dimension Clusters\nFaster R-CNN에서는 anchor box의 ratio와 size를 미리 정해주었는데, YOLO v2에서는 K-means Clustering을 통해 ground truth와 유사한 optimal anchor box를 탐색하였다. 이때 euclidian 거리 대신 IoU를 기준으로 clustering하는데, 이는 bounding box의 중심 거리와 높이, 너비 등을 모두 고려하기 위함이다.\n1.1.5. Direct Location Prediction\nBounding Box Regression 과정에서 bounding box의 중심 좌표를 다음과 같이 sigmoid를 통해 계산하여 grid cell을 벗어나지 않도록 하였다.\nbx​by​​=σ(tx​)+cx​=σ(ty​)+cy​​​\n1.1.6. Fine-Grained Features\n13×13 feature map으로는 작은 객체를 검출하기 어렵기 때문에 26×26 feature map을 resize하여 concatenate하는 passthrough layer를 추가하였다.\n1.1.7. Multi-Scale Triaining\nfully connected layer 대신 convolutional layer를 사용하기 때문에 input size가 고정적이지 않아도 되다. 따라서 10 batch 마다 input size를 랜덤하게 변경하여 학습시켰다.\n1.2. Faster\n1.2.1. DarkNet-19\n1.2.1.1. Global Average Pooling\n1.3. Stronger\n1.3.1. Hierarchial Classification\ndirected graph 형태의 ImageNet을 공통 root를 갖는 label끼리 묶어 WordTree를 구성한 뒤 classification 과정에서 조건부 확률을 통해 loss를 구할 수 있도록 하였다.\n1.3.2. Joint Classification and Detection\nobject detection을 위한 이미지를 학습시키는 경우 평소와 같이 loss를 사용하여 backpropagation을 수행하며 classification loss는 앞서 구성한 WordTree를 사용한다. classification 이미지에 대해서는 ground truth와의 IoU가 0.3 이상인 경우에만 classification loss에 대한 backpropagation을 수행한다."},"정보과학/Machine-Learning/Curse-of-Dimentionality":{"title":"Curse of Dimentionality","links":["정보과학/Machine-Learning/model/Decision-Tree","정보과학/Machine-Learning/model/K-Nearest-Neighbor","정보과학/Machine-Learning/Learning-Strategy/Regularization","정보과학/Dimension-Reduction/Principal-Component-Analysis"],"tags":[],"content":"고정된 크기의 데이터셋이 차원이 늘어남에 따라 특성 공간이 희소해지는 현상이다. Decision Tree나 K-Nearest Neighbor와 같은 모델에서는 Regularization을 적용하기 어렵기 때문에 특성 선택과 Principal Component Analysis와 같은 차원 축소 기법을 주로 사용한다."},"정보과학/Machine-Learning/Learning-Strategy/Batch":{"title":"Batch","links":[],"tags":[],"content":"Neural Net을 학습시킬 때 많은 라이브러리들이 배열을 효율적으로 연산할 수 있도록 최적화 되어있기 때문에 전체 데이터에 대해 하나씩 연산을 수행하는 것 보다 batch 단위로 학습시키는 것이 더 효율적이다.\nMini-Batch\n학습 데이터를 N개로 분할하여 각각의 mini-batch에 대해 gradient descent를 수행한다. 데이터의 일부를 이용해 학습하므로 수렴 과정이 안정적이지는 않지만 학습 속도가 빠르다."},"정보과학/Machine-Learning/Learning-Strategy/Early-Stopping":{"title":"Early Stopping","links":[],"tags":["작성중"],"content":""},"정보과학/Machine-Learning/Learning-Strategy/Ensemble":{"title":"Ensemble","links":[],"tags":[],"content":"여러 classifier를 하나의 meta classifier로 연결하여 개별적인 classifier보다 더 좋은 일반화 성능을 달성하는 것이다.\nMajority Voting\nclassifier의 과반수가 예측한 class label을 선택하는 단순한 방법이다.\nPlurality Voting\nbinary classification에 사용하는 majority voting을 multiclass classification으로 확장한 것으로 가장 많은 투표를 받은 label을 선택한다.\nWeighted Voting\nmajoriy voting에 가중치를 도입한 방식으로 각 모델의 정확도를 곱하여 결과를 취합하는 방식이다.\nBagging\nbootstrap aggregating의 약자로, dataset에서 중복을 허용하는 bootstrap sample을 뽑아서 여러 개의 ensemble model을 학습시키는 방법이다. 복원추출을 수행함으로써 기존의 데이터가 갖는 noise와 다른 noise를 갖는 dataset을 만들어내어 특정한 noise에 종속되어 noise의 변동으로 인한 영향을 방지하는 역할을 한다.\nBoosting\ntrain dataset에서 중복을 허용하지 않는 부분 집합 d1​을 추출해 weak learner C1​을 학습시킨다. 이후 d2​와 C1​에서 잘못 분류된 샘플의 50%를 더하여 C2​를 학습시킨다. C1​과 C2​에서 잘못 분류한 샘플 d3​를 찾아 C3​를 학습 시킨 뒤 C1​, C2​, C3​를 plurality voting으로 연결한다.\nAdaBoost"},"정보과학/Machine-Learning/Learning-Strategy/Hard-Negative-Mining":{"title":"Hard Negative Mining","links":[],"tags":[],"content":"모델이 예측에 실패하는 어려운(hard) sample들을 모으는 기법으로, hard negative mining을 통해 수집된 데이터를 활용하여 모델을 보다 강건하게 학습시키는 것이 가능해진다. 예를 들어 이미지에서 사람의 안면의 위치를 탐지하는 모델을 학습시킨다고 할 때, 사람의 안면은 positive sample이며, 그 외의 배경은 negative sample이다. 이 때 모델이 배경이라고 예측했으며, 실제로 배경인 bounding box는 True Negative에 해당하는 sample이다. 반면에 모델이 안면이라고 예측했지만, 실제로 배경인 경우는 False Positive sample에 해당한다.\n모델은 주로 False Positive라고 예측하는 오류를 범한다. 이는 객체 탐지 시, 객체의 위치에 해당하는 positive sample보다 배경에 해당하는 negative sample이 훨씬 많은 class imbalance로 인해 발생하며 이러한 문제를 해결하기 위해 모델이 잘못 판단한 False Positive sample을 학습 과정에서 추가하여 재학습하면 모델은 보다 강건해지며, False Positive라고 판단하는 오류가 줄어든다."},"정보과학/Machine-Learning/Learning-Strategy/Normalization":{"title":"Normalization","links":["생명과학/Neuroscience/Lateral-Inhibition"],"tags":[],"content":"Batch Normalization\nSigmoid의 경우 입력 값의 크기가 커질 수록 gradient의 크기가 작아지고 이러한 노드가 누적되며 Vanishing gradient가 발생한다. 이러한 문제를 해결하기 위해 ReLU를 사용한다고 가정했을 때, 만약 입력 값이 모두 양수라면, 또는 모두 음수라면 ReLU는 linear한 함수와 구분할 수 없게 되어 비선형성이 가지는 이점을 활용할 수 없게 될 것이다.\n이때 활용할 수 있는 가장 단순한 해법 중 하나가 normalization이다. ReLU의 경우 비선형성을 가지는 부분은 결국 입력 값이 0인 지점이기에 평균을 0, 분산을 1로 맞추어 간단히 해결할 수 있다. 하지만 activation function은 종류가 많고 각 함수에 적절한 평균, 분산도 알 수 없기에 이를 적절히 fitting하기 위한 방법 또한 필요하다. 입력 데이터 x의 평균을 xˉ, 표준 편차를 σ라고 한다면 각각의 feature에 대해 다음과 같이 2개의 parameter a, b를 통해 fitting 할 수 있다.\na(σx−xˉ​)+b\n위와 같은 방식을 통해 training을 마치고 실제 prediction을 진행할 때에는 대부분의 상황에서 입력 값이 하나씩 들어온다. 이런 상황에서 평균은 입력 값이 될 것이고 분산은 0이 되므로 train set에서 비교적 최근 값에 더 큰 가중치를 부여하는 exponential moving everage를 통해 weighted average를 구한 뒤 이를 통해 얻은 parameter를 사용한다.\n앞선 layer의 parameter 변화 또한 hidden layer와 output layer에서는 입력 데이터의 분포에 영향을 줄 수 있기 때문에 이러한 현상을 방지하기도 한다.\nBatch Normalization의 parameter는 어떻게 gradient를 계산할까\nLayer Normalization\nLayer Normalization은 각각의 layer가 비선형성을 잃지 않는 지를 중요시한다. 따라서 layer에 들어오는 하나의 데이터에 대해서 normalization을 수행한다.\nLocal Response Normalization\nLateral Inhibition을 모방하여 만들어진 방법으로, 이웃한 feature map에서 특정 위치의 값이 크면, local response normalization이 적용된 feature map의 해당 위치의 값을 작게 한다.ㄴㄴ"},"정보과학/Machine-Learning/Learning-Strategy/Optimizer":{"title":"Optimizer","links":["정보과학/Machine-Learning/Learning-Strategy/Batch","정보과학/Machine-Learning/Learning-Strategy/Early-Stopping"],"tags":[],"content":"Gradient Descent\n가중치를 업데이트할 때 기울기의 크기에 비례하여 반대 방향으로 가중치를 조절한다.\nωt+1​=ωt​−α∇f(xt​)\nStochastic Gradient Descent\n주어진 data 내에서 batch size가 1인 Mini-Batch를 무작위적으로 선택하여 gradient descent를 수행한다. batch size가 작기 때문에 수렴 과정이 매우 불안정하며 이를 해결하기 위해 batch size를 조절한 mini-batch SGD가 있다.\nMomentum\ngradient descent와 달리 gradient를 가속도로 사용한다. 따라서 parameter 업데이트를 위한 속도 v를 다음과 같이 정의한다.\nvt​ωt+1​​=mvt−1​−αδwδloss​=ωt​+vt​​\n위 식에서 알 수 있듯이 gradient가 0이어도 vt​=mvt−1​이 되기 때문에 saddle point나 local minima로 인한 문제를 어느 정도 개선할 수 있다.\nRMSprop\ngt​ωt​​=γgt−1​+(1−γ)(∇f(xt−1​))2=ωt−1​−gt​​+ϵα​∇f(xt−1​)​​\ngt​는 gradient의 exponentially weighted average of squares로 γ를 통해 이전 값에 얼마나 영향을 받을 지를 결정할 수 있으며 따라서 gt​+ϵ​은 gradient의 RMS와 유사한 식이 된다. 여기서 ϵ은 분모가 0에 가까워지는 것을 방지하기 위한 값이다. ∣Δx∣는 최근의 gradient가 크면 learning rate를 줄여주고 gradient가 작으면 learning rate를 키워주기 때문에 학습 과정에서 진동을 줄이고 Early Stopping을 방지하는 데에 도움이 된다.\nAdam\nmomentum과 RMSprop이 합쳐진 형태로, 학습 속도가 관성을 가지며 최근의 gradient에 따라 learning rate가 변하는 학습 방식이다.\nvt+1​γt+1​xt+1​​=β1​vt​+(1−β1​)∇f(xt​)=β2​γt​+(1−β2​)(∇f(xt​))2=xt​−γt+1​​+ϵα​vt+1​​​"},"정보과학/Machine-Learning/Learning-Strategy/Regularization":{"title":"Regularization","links":["정보과학/Machine-Learning/Learning-Strategy/Ensemble","정보과학/Machine-Learning/Learning-Strategy/Batch","정보과학/Machine-Learning/Natural-Language-Processing/Word-Embedding"],"tags":[],"content":"Dropout\n하나의 Neural Network에서 일부 node를 확률적으로 생략하게 되면 여러 모델을 학습시켜 Ensemble 하는 것과 유사한 효과를 낼 수 있다.\nγσF(x)​∼probability=activation function=σ(w∗γx+b)​​\nDropout Net은 일반적인 Neural Network와 동일한 방식으로 학습할 수 있으며 단지 Mini-Batch를 사용한 학습 과정에서 일부 node를 dropout하여 나타난 network를 학습시킨다는 차이가 있다.\nLabel Smoothing\nOne-Hot Encoding의 경우 하나의 값만 1이고 나머지는 0으로 나타내는데, 이를 조금이나마 부드럽게 만드는 것을 label smoothing이라고 한다. K개의 class에 대해서 i번째 차원에 대한 smoothing vector의 값은 다음과 같다.\nyiLS​=yi​(1−α)+Kα​\n[1]에서는 one-hot encoding을 활용한 학습 방식에서는 올바른 class의 구간에 가까운 벡터를 만들어내고자 하지만, label smoothing을 사용하게 되면 잘못된 class의 구간과 일정한 거리를 유지하여 class 간의 거리가 일정하고 동일한 class내에서는 가까운 위치에 배치되도록 하는 효과를 가지는 것을 보였다.\nWeight Decay\nL2 Regularization\nReferences\n[1] Müller, Rafael, Simon Kornblith, and Geoffrey E. Hinton. “When does label smoothing help?.” Advances in neural information processing systems 32 (2019).(link)"},"정보과학/Machine-Learning/Learning-Strategy/Teacher-Forcing":{"title":"Teacher Forcing","links":["정보과학/Machine-Learning/Architecture/Encoder-and-Decoder"],"tags":[],"content":"주로 Encoder&amp;Decoder 구조에서 사용되는 학습 방식으로, 예를 들어 문장 번역 시 초기에 잘못된 단어를 사용하여 번역된 경우 이후에 번역되는 모든 단어에 영향을 끼치기 때문에 예측 대상이 되는 단어 이전의 Ground Truth를 제공한 뒤 다음 단어를 예측하는 방식으로 학습시킨다."},"정보과학/Machine-Learning/Method/Kernel-Method":{"title":"Kernel Method","links":[],"tags":[],"content":"Kernel Trick의 기본 아이디어는 매핑 함수 ϕ를 사용하여 원본 특성의 비선형 조합을 선형적으로 구분 가능한 고차원 공간에 투영하는 것이다.\nϕ(x1​, x2​)=(z1​, z2​, z3​)=(x1​, x2​, x12​+x22​)\n아래와 같이 데이터를 투영시킴으로써 구분 가능한 hyperplane을 만들어 낼 수 있다.\n"},"정보과학/Machine-Learning/Method/Positional-Encoding":{"title":"Positional Encoding","links":["정보과학/실습/Sinusoidal-Positional-Encoding.py"],"tags":[],"content":"단어를 embedding vector 형식으로 바꾸는 경우 vector에는 문장 내 에서의 단어의 위치에 관한 정보가 저장되어 있지 않기 때문에  각 위치마다 특정 벡터를 더하여 단어 위치 정보를 저장한다.\nSinusoidal Positional Encoding\n문장 내에서 단어의 위치 pos, embedding vector의 i번째 차원, 모델의 차원 dmodel​에 대해 아래와 같은 식을 통해 positional encoding을 위한 벡터를 구한다. 여기서  i가 embedding vector의 차원에 영향을 받기 때문에 각 차원마다 다른 주기의 함수를 사용하며 pos를 통해 위치 정보를 어느 정도 encoding 할 수 있다.\ns.t. d mod 2≡0PE(pos,2i)=sin(100002i/dmodel​pos​)PE(pos,2i+1)=cos(100002i/dmodel​pos​)​\n그리고 나서 PE를 기존 embedding vector에 더해주어 위치 정보를 추가한다.\n위 식을 통해 알 수 있는 점은, n번째 단어의 positional encoding vector를 통해 n+k번째 단어의 positional encoding vector를 유추할 수 있다는 점이다. 단어의 위치가 pos와 pos+k일 때의 PE vector는 아래와 같다.\nωi​=100002i/dmodel​1​\nPEpos,i​=[sin(pos⋅ωi​),cos(pos⋅ωi​)]​\nPEpos+k,i​=[sin((pos+k)⋅ωi​),cos((pos+k)⋅ωi​),]​\nPEpos+k​를 자세히 보면 ω0​,ω1​,ω2​…일 때 각각 k⋅ωi​만큼 회전 변환 해주었음을 알 수 있다. 따라서 PEpos+k​는 다음과 같이 나타낼 수 있다.\nT(k)⋅PEpos​=PEpos+k​\nM(k)i​=(cos(k⋅ωi​)−sin(k⋅ωi​)​sin(k⋅ωi​)cos(k⋅ωi​)​)​\nT(k)=​(cos(k⋅ω0​)−sin(k⋅ω0​)​sin(k⋅ω0​)cos(k⋅ω0​)​)⋮0​…⋱…​0⋮(cos(k⋅ωdmodel​/2​)−sin(k⋅ωdmodel​/2​)​sin(k⋅ωdmodel​/2​)cos(k⋅ωdmodel​/2​)​)​​\n따라서 sinusoidal positional encoding은 문장 내에서 상대적인 위치 정보를 벡터의 회전을 통해 저장하고 있다고 할 수 있다.\nSinusoidal-Positional-Encoding.py"},"정보과학/Machine-Learning/Natural-Language-Processing/CoVe":{"title":"CoVe","links":[],"tags":["작성중"],"content":""},"정보과학/Machine-Learning/Natural-Language-Processing/Deep-Passage-Retrieval":{"title":"Deep Passage Retrieval","links":[],"tags":["작성중"],"content":""},"정보과학/Machine-Learning/Natural-Language-Processing/Word-Embedding":{"title":"Word Embedding","links":[],"tags":[],"content":"One-Hot Enoding\n서로 다른 단어들에 고유한 숫자가 부여된 단어 집합에서 특정 단어에 해당하는 숫자를 1, 나머지를 0으로 만들어 단어를 vector로 표현하는 방식이다. 단어 집합 처리하고자 하는 모든 단어를 포함하고 있어야 하고 단어 간의 유사도를 표현하지 못한다는 단점이 있다.\nBag-of-Word\n전체 문서에 대해 고유한 token, 예를 들어 단어로 이루어진 vocabulary를 작성하여 특정 문서에 각 단어가 얼마나 자주 등장하는 지를 통해 문서의 특성 벡터를 만든다."},"정보과학/Machine-Learning/Natural-Language-Processing/model/BERT":{"title":"BERT","links":[],"tags":["작성중"],"content":""},"정보과학/Machine-Learning/Natural-Language-Processing/model/ELMo":{"title":"ELMo","links":["정보과학/Machine-Learning/Learning-Strategy/Normalization","정보과학/Machine-Learning/Computer-Vision/model/ResNet"],"tags":["작성중"],"content":"\n1. Architecture\n1.1. Biderectional LSTM\nRk​={xkLM​,hk,jLM​hk,jLM​∣j=1,2,…,L}\n각 LSTM layer의 hidden state를 word embedding과 함께 concatenate하여 2L+1개의 representation을 얻는다. 이를 weighted sum하여 하나의 embedding vector를 얻을 수 있다. γ는 scaler parameter로 Layer Normalization을 형성하는 데에 관여한다.\nELMOktask​=E(Rk​;Θtask)=γtaskj=0∑L​sjtask​hk,jLM​\n1.1.1. Skip Connection\n2. Training\n3. Application"},"정보과학/Machine-Learning/Natural-Language-Processing/model/GPT/GPT":{"title":"GPT","links":[],"tags":["작성중"],"content":""},"정보과학/Machine-Learning/Natural-Language-Processing/model/Transformer":{"title":"Transformer","links":["정보과학/Machine-Learning/Architecture/Encoder-and-Decoder","정보과학/Machine-Learning/Architecture/Attention","정보과학/Machine-Learning/Architecture/Feed-Forward-Network","정보과학/Machine-Learning/Architecture/Skip-Connection","정보과학/Machine-Learning/Learning-Strategy/Normalization","정보과학/Machine-Learning/Learning-Strategy/Teacher-Forcing","정보과학/Machine-Learning/Method/Positional-Encoding","정보과학/Machine-Learning/Learning-Strategy/Optimizer","정보과학/Machine-Learning/Learning-Strategy/Regularization"],"tags":[],"content":"1. Architecture\n1.1 Encoder&amp;Decoder\n1.1.1 Encoder\nEncoder는 6개의 stack이 동일한 layer로 구성되어 있으며 각 layer는 2개의 sub-layer를 가진다.\nsub-layer는 Multi-Head Self-Attention과 Position-Wise FFN이다. 2개의 sub-layer에는 각각 Skip Connection을 도입한 뒤 Layer Normalization하였다. 따라서 각 sub-layer의 연산은 다음과 같이 나타낼 수 있다.\nLayerNorm(x+Sublayer(x))\nresidual connection을 사용하기 위해서, 모델 내에 포함된 모든 sub-layer는 embedding layer와 같은 차원의 출력을 만들어내야 한다. (dmodel​=512)\n1.1.2 Decoder\nEncoder는 6개의 stack이 동일한 layer로 구성되어 있으며 각 layer는 3개의 sub-layer를 가진다.\n2개의 sub-layer의 구성은 Encoder와 동일하며 나머지 하나의 sub-layer는 Encoder의 출력에 대해 multi-head attention을 수행한다. Encoder와 마찬가지로 각 sub-layer에 residual connection 및 layer normalization을 도입하였다.\n1.2. Attention\n1.2.1. Scaled Dot-Product Attention\n1.2.2. Multi-Head Attention\n1.2.3. Masked Attention\ntransformer의 경우 문장을 순차적으로 번역하는데 이때 학습을 병렬적으로 진행하기 위해 Teacher Forcing을 사용하기 때문에 Encoder에는 입력 문장을, Decoder에는 현재까지 번역된 문장을 제공하게 된다. 이때 Decoder에 입력된 정보는 self-attention을 거치게 되는데, 만약 추후에 사용될 Ground Truth가 포함되어 있다면 이들 또한 attention 과정에서 query를 표현하기 위한 벡터로 사용되기 때문에 학습에 문제가 생길 수 있다. 이를 해결하기 위해 아직 번역되지 않은 부분의 내적을 특정 값으로 masking하여 학습에 사용되지 않도록 한다.\n1.3. Position-Wise Feed-Forwaed Networks\n1.4. Positional Encoding\n2. Training\n2.1. Adam Optimizer\nlrate=dmodel−0.5​⋅min(step_num−0.5,step_num⋅warmup_step−1.5)\n위와 같은 식을 통해 warmup_steps에 도달할 때까지 learning rate이 선형적으로 증가하여 학습 초기에 global minima를 찾고 이후 learning rate이 점차 감소하여 수렴하는 형태로 탐색할 수 있다.\n2.2. Regularization\n2.2.1. Residual Dropout\nencoder와 decoder 모두 각 sub-layer의 output 및 embedding vector에 positional encoding을 더하는 과정에 Dropout을 적용하였다.\n2.2.2 Label Smoothing\n질문\nTransformer의 Decoder에서 Masking을 수행하더라도 입력 데이터를 통해 단어의 개수를 알 수 있는데, 이는 어떻게 해결하는가?\n애초에 다음에 올 단어만 예측하는 거니까 EOS 기준으로 판단?\n또는 dmodel​에 맞추어 padding이 존재하는가?\nReferences\n[1] Vaswani, Ashish, et al. “Attention is all you// need.” Advances in neural information processing systems 30 (2017).(link)\n[2] Attention Is All You Need 번역 및 정리 (Transformer)(link)\n[3] Transformer: Attention Is All You Need - 꼼꼼한 딥러닝 논문 리뷰와 코드 실습(link)\n[4] Attention/Transformer 시각화로 설명(link)"},"정보과학/Machine-Learning/Natural-Language-Processing/pretrained-word-vector":{"title":"pretrained word vector","links":[],"tags":["작성중"],"content":""},"정보과학/Machine-Learning/Validation/Holdout-Cross-Validation":{"title":"Holdout Cross-Validation","links":[],"tags":[],"content":"dataset을 train과 test로 나눈 뒤 각각 모델 훈련과 일반화 검증을 위해 사용한다. 모델 선택에 holdout을 사용하는 가장 좋은 방법은 train, test, validation 세 가지로 나눈 뒤 모델 학습 과정에서는 train 및 validation dataset을 사용하고 test dataset을 통해 일반화 검증을 수행한다."},"정보과학/Machine-Learning/Validation/K-Fold-Cross-Validation":{"title":"K-Fold Cross-Validation","links":["정보과학/Machine-Learning/Validation/Holdout-Cross-Validation"],"tags":[],"content":"dataset을 중복을 허용하지 않는 k개의 fold로 랜덤하게 나눈다. 그 뒤 k-1개의 fold를 train, 나머지 하나의 fold를 test로 사용하여 성능을 평가한다. 이 과정을 k번 반복하여 k개의 모델과 성능 추정을 얻는다. 독립적인 fold에서 얻은 성능 추정을 기반으로 모델의 평균 성능을 계산한다. 간단히 정리하자면, k번의 Holdout Cross-Validation을 수행한 뒤 평균을 구하는 것이다.\nStratified K-Fold Cross Validation\n각 fold에서의 class 비율이 전체 dataset에 있는 class 비율을 유지하도록 한다."},"정보과학/Machine-Learning/model/Adaline":{"title":"Adaline","links":["정보과학/Machine-Learning/model/Perceptron","정보과학/실습/Adaline.py"],"tags":[],"content":"Adaptive Linear Neuron은 Perceptron의 향상된 버전으로 cost function을 연속적으로 정의하고 최소화하는 핵심 개념을 보여준다. 기존의 perceptron에서는 계단 함수를 사용한 반면에 선형 활성화 함수를 사용한다. Adaline의 선형 활성화 함수는 다음과 같다.\nϕ(wTx)=wTx\n이러한 함수가 가중치 학습 과정에 사용되기는 하지만, 여전히 최종 출력에는 임계 함수를 사용한다.\nTransclude of Adaline.py"},"정보과학/Machine-Learning/model/Decision-Tree":{"title":"Decision Tree","links":[],"tags":["작성중"],"content":"Decision Tree는 훈련 데이터에 있는 feature를 기반으로 class label을 추정할 수 있는 조건을 학습한다."},"정보과학/Machine-Learning/model/K-Nearest-Neighbor":{"title":"K-Nearest Neighbor","links":["정보과학/Data-Structure/k-d-Tree"],"tags":[],"content":"KNN은 다음과 같은 단계로 요약할 수 있다.\n\n이웃 수 k와 거리 측정 기준을 선택한다.\n분류하려는 샘플에서 k개의 최근접 이웃을 찾는다.\n투표를 통해 class label을 결정한다.\n\n이러한 메모리 기반의 classifier는 새로운 데이터에 즉시 적응할 수 있다는 장점이 있다. 하지만 데이터가 추가됨에 따라 최악의 경우 계산 비용이 선형적으로 증가할 수 있다. 이러한 문제를 해결하기 위해 k-d Tree와 같은 구조를 사용한다."},"정보과학/Machine-Learning/model/Kernel-Support-Vector-Machine":{"title":"Kernel Support Vector Machine","links":["정보과학/Machine-Learning/model/Support-Vector-Machine","정보과학/Machine-Learning/Method/Kernel-Method","정보과학/실습/Kernel_Support_Vector_Machine.ipynb"],"tags":["작성중"],"content":"Support Vector Machine으로 비선형 문제를 해결하기 위해 Kernel Method를 사용하여 고차원 특성 공간으로 변환한다. 그다음 이 새로운 특성 공간에서 데이터를 분류하는 support vector machine을 훈련시킨다.\nf(xi​, xj​)=ϕ(xi​)Tϕ(xj​)\n이러한 매핑 방식의 한 가지 문제점은 데이터를 고차원으로 매핑한 뒤 내적을 계산하기 때문에 계산 비용이  높다는 것이다. 이를 해결하기 위해 kernel trick이 사용된다.\nKernel Trick\n가장 널리 사용되는 kernel 중 하나는 gaussian kernal이다. 이를 통해 연산이 낮은 차원을 통해 진행된 다.\nf(xi​, xj​)=exp(−2σ2∣∣xi​−xj​∣∣2​)\nKernel_Support_Vector_Machine.ipynb"},"정보과학/Machine-Learning/model/Long-Short-Term-Memory":{"title":"Long Short-Term Memory","links":[],"tags":["작성중"],"content":"Bidirectional LSTM"},"정보과학/Machine-Learning/model/Perceptron":{"title":"Perceptron","links":[],"tags":["작성중"],"content":""},"정보과학/Machine-Learning/model/Support-Vector-Machine":{"title":"Support Vector Machine","links":["정보과학/실습/Support_Vector_Machine.ipynb"],"tags":["작성중"],"content":"주어진 데이터를 바탕으로 두 카테고리 사이의 가장 큰 마진을 갖는 경계를 찾는 이진 분류기다.\nTransclude of Support_Vector_Machine.ipynb"},"정보과학/Machine-Learning/자료":{"title":"자료","links":[],"tags":[],"content":"한땀한땀 딥러닝 컴퓨터 비전 백과사전\nhttps://wikidocs.net/149469"},"정보과학/Pre-Processing/Whitening-Transform":{"title":"Whitening Transform","links":["정보과학/Dimension-Reduction/Principal-Component-Analysis"],"tags":["작성중"],"content":"Principal Component Analysis를 통해 feature들 간의 상관관계를 제거한 뒤 standardization하여 평균이 0이고 공분산을 단위 행렬로 갖는 정규분포 형태로 변환한다."},"화학/Conjugation":{"title":"Conjugation","links":[],"tags":["작성중"],"content":"이중 또는 삼중 결합이 탄소-탄소 단일 결합과 교대로 있어 전자가 여러 분자에 걸쳐 비편재화된 분자 또는 분자의 부분으로, 콘쥬게이션된 분자는 각 원자에 존재하는 p 오비탈에 위치한 전자가 비편재화되어 비콘쥬게이션 분자보다 안정하다."},"화학/Electrocyclic-reaction":{"title":"Electrocyclic reaction","links":["화학/Conjugation"],"tags":["작성중"],"content":"1.Electrocyclic reaction\n전자 고리화 반응의 기본적 원리는 반응기의 회전(rotatory)이다. 처음 반응할 때의 반응물의 배열이 어떻게 되어있는지, 열화학적 아니면 광화학적으로 일어나는지에 따라 생성물의 입체화학은 완전히 다르게 된다. 반응물의 배열은 기본적으로 주어진 것이므로, 반응단계의 메커니즘을 보아야한다.\n1.1 Conjugation\n1.2 Thermochemical Stereochemistry\n열역학적으로는 반응물이 안정한 쪽으로 일어난다. 먼저 벤젠의 π궤도함수를 살펴보자.\n\n벤젠의 π궤도함수는 π전자의 겹침에 의해 일어나는데 각각의 탄소에는 π궤도함수가 1개씩 있어 벤젠은 총 6개의 π궤도함수를 가지게 된다. 이 π궤도함수의 겹침을 정리하면 왼쪽그림과 같이 3개의 결합성 궤도함수와 3개의 반결합성 궤도함수를 가지게 된다. 결합성 궤도함수중 제일 높은 에너지를 가진 궤도함수를 HOMO(Highest Occupied Molecular Orbital), 반결합성 궤도함수중 제일 낮은 궤도함수 겹침을 LUMO(Lowest Unoccupied Molecular Orbital)라고 한다. 결합성 궤도함수의 제일 아래 궤도함수는 부호가 모두 같은 방향을 향하고 있어 제일 안정적인 궤도함수가 된다. 궤도함수 겹침이 적어질수록 벤젠의 궤도함수의 에너지는 불안정하게 된다. 이렇게 여러 개가 나타날 수 있는 것은 벤젠이 혼성궤도함수를 가지기 때문이다. 원래 반응을 비교할 때에는 모든 궤도함수를 봐야하지만, 교토대학의 Fukui 교수는 HOMO와 LUMO를 살펴보면 된다고 하였다. 전자 고리화 반응에서는 HOMO를 살펴보면 된다.\n벤젠이 아닌 다이 엔이나 트라이 엔의 고리 형성 반응을 볼 것이므로 양 끝의 π궤도함수를 유심히 봐야한다.\n만일 반응물이 다이 엔이나 짝수 엔이라면 반응은 다음과 같이 된다.\n \n즉 p궤도함수의 회전이 동일한 방향으로 일어나게 된다. 반대로 트라이 엔이나 홀수 엔은,\n \n위 그림과 같이 반대방향으로 회전하게 된다.\n1.2 Photochemical Stereochemistry\n광화학적 고리화 반응에서 중요한 것은 궤도함수의 HOMO에 있는 전자가 자외선을 받고 LUMO로 들뜨게 된다는 것이다. 즉 전자의 대칭성이 완전히 바뀌게 된다. 즉, 다이 엔이나 짝수 엔은\n \n이렇게 마디가 1개였던 궤도함수가 2개로 변한다. 그래서 일반적 고리화반응의 트라이 엔의 반응방법인 반대방향 회전이 일어나게 된다.\n마찬가지로 트라이 엔이나 홀수 엔은,\n \n역시 마디가 2개였던 궤도함수가 3개로 변한다. 또한 고리 화 반응이 다이 엔의 일반적 반응방법인 동일방향 회전이 일어나게 된다. 정리하면 다음과 같다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n전자쌍 개수π궤도함수열역학적 반응광화학적 반응짝수반대면동일방향반대방향홀수동일면반대방향동일방향\n\nCycloaddition reaction(고리화 첨가 반응)\n\n앞의 반응이 분자 내에서의 고리화 반응이었다면, 이번반응은 불포화분자가 서로 첨가되는 반응이다. 또 앞의 반응이 분자 내에서 π궤도함수가 자체적으로 회전이 되면서 결합하여 입체화학이 바뀌었지만, 이번 반응은 서로 다른 반응물의 π궤도함수가 회전하면서 반응하지는 않고 겹침에 의해 반응이 진행된다. 이 반응의 중요한 특징은 한 화합물의 HOMO의 전자가 다른 화합물의 LUMO의 빈 궤도함수에 전자를 제공한다는 것이다. 생성물의 안정성 등 반응성을 판단하는 데에는 다른 변수가 많겠지만 불포화분자의 서로 첨가반응은 위에서와 같이 π궤도함수의 겹침에 일어나기 때문에 주요한 요인은 이 궤도함수 겹침에 있다. 역시 위에서의 원리와 동일하게 반응물의 π궤도함수가 서로 잘 맞물리면 반응이 일어나게 되고 반대부호로 겹치면 반응이 잘 일어나지 않게 된다. 한 예로 Diels-Alder 반응이 있다.\n \n다이 엔이 알켄에 전자를 준다고 생각하면 다이 엔은 HOMO인 반대면 궤도함수가 되고, 알켄은 LUMO인 반대면 궤도함수가 된다. 즉 동일면(Superficial) 고리화 첨가반응이 일어나 반응이 잘 일어날 수 있게 된다.\n이제 Cyclobutane 고리형성반응을 보자.\n \n알켄의 HOMO는 동일면이고, LUMO는 반대 면이기 때문에 반응은 잘 일어나지 않는다. 그러므로 자외선을 가해 한 화합물의 입체화학을 변형시킨 후 반응시켜야 반응이 잘 일어나게 된다.\n이 반응 또한 열역학적, 광화학적으로 설명될 수 있다. 생각대로 정리해보았다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n전자쌍개수열역학적 반응광화학적 반응짝수+짝수잘일어나지않음잘 일어남짝수+홀수잘 일어남잘일어나지않음홀수+홀수잘일어나지않음잘 일어남\n2.1 OO\n2.2 OO\n\nSigmatropic rearrangement(시그마결합자리옮김 반응)\n\n앞의 두 반응이 결합을 생성하는 반응이었다면, 이번반응은 치환기의 σ결합이 π전자계를 가로질러 이동하는 고리형 협동반응이다. 동일면 자리 옮김과 반대면 자리 옮김이 있는데, 동일면자리옮김반응은 다음과 같다.\n\n이렇게 치환기와 같은 방향에 있는 π궤도함수 쪽으로 옮겨가는 반응이고, 반대면 자리옮김반응은 다음과 같다.\n\n치환기와 반대방향에 있는 π궤도함수 쪽으로 옮겨가는 반응이다. 역시 동일면으로 자리 옮김을 하는 게 효율이 훨씬 좋고 잘 일어날 것이다.\n그렇다면 다음반응을 보자.\n\n이 반응은 1,5 자리옮김반응인데 이 반응의 궤도함수 겹침은 다음과 같이 된다.\n\n단일결합이 붙어있는 탄소의 p궤도함수는 분자평면에 평행하게 되고, 이중결합의 p궤도함수는 π궤도함수로 수직하게 있게 되는데 동일면과 반대면의 중간 위치정도의 반응성을 가지게 된다.\nconjugated된 자리옮김반응은 이렇게 설명할 수 있다. 하지만 nonconjugated된 다이 엔의 반응은 어떨까. 다음을 보자.\n\nπ궤도함수가 이동하면서 공명이 안 되기 때문에 단일결합을 끊게 된다. 그러면서 allyl기 2개로 나누어지게 된다.\n\n이 반응은 전자의 이동으로만 본다면 간단하게 표현이 되지만 실제 반응메커니즘은 매우 복잡하다. 그것을 설명하기 위해 1952년 Kenichi Fukui에 의해 Frontier Molecular Orbital Analysis(FMO) 이론이 제안되었다. 이 이론은 다음과 같다.\n\n\n서로 다른 분자의 채워진 오비탈들은 서로 반발한다.\n\n\n한 분자의 양전하는 다른 음전하를 끌어당긴다.\n\n\n한 분자의 채워진 오비탈(HOMO)과 다른 분자의 채워지지 않은 오비탈(LUMO)은 서로 끌어당겨 상호작용한다.\n\n\n이 이론에 의해서 1,5 다이 엔은 HOMO Allyl과 LUMO Allyl로 나누어지게 된다.\n\n이렇게 나누어진 2개의 HOMO, LUMO Allyl은 σ결합 자리옮김반응에 의해서 위치가 바뀌게 되고 다시 결합되거나, 결합이 약해진 부분의 반대쪽 부분이 결합되는 방법으로 메커니즘은 설명될 수 있다.\n이 반응을 Cope Rearrangement라고 한다. 또한 Allyl기의 한쪽 C가 O로 치환된 걸로 반응시킬 때, 이 반응을 Claisen Rearrangement라고 부른다.\n"},"화학/개인-공부":{"title":"개인 공부","links":[],"tags":[],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLINKpyMCD: Python package for searching transition states via the multicoordinate driven methodOn the importance of the electric double layer structure in aqueous electrocatalysis "}}